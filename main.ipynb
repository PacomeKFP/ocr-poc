{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "cfb932ad",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Informations extraites:\n",
                        "nom: KENGALIFEGUE\n",
                        "prenom: PACOME\n",
                        "date_naissance: 29.06.2004\n",
                        "lieu_naissance: TAILLE/HEIGHT\n",
                        "sexe: M\n",
                        "taille: 1,75\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "import re\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "class CNIExtractorKeywordProximity:\n",
                "    def __init__(self):\n",
                "        self.label_mappings = {\n",
                "            'nom': ['NOM', 'SURNAME', 'N0M'],  # N0M pour gérer erreur OCR O/0\n",
                "            'prenom': ['PRENOMS', 'PRENOM', 'GIVEN NAMES', 'GIVEN NAME'],\n",
                "            'date_naissance': ['DATE DE NAISSANCE', 'DATE OF BIRTH', 'DATEDE NAISSANCE'],\n",
                "            'lieu_naissance': ['LIEU DE NAISSANCE', 'PLACE OF BIRTH', 'LIEUDE NAISSANCE'],\n",
                "            'sexe': ['SEXE', 'SEX'],\n",
                "            'taille': ['TAILLE', 'HEIGHT', 'SIZE']\n",
                "        }\n",
                "        \n",
                "    def calculate_center(self, polygon: List[List[int]]) -> Tuple[float, float]:\n",
                "        \"\"\"Calcule le centre d'un polygone.\"\"\"\n",
                "        x_coords = [p[0] for p in polygon]\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        return (sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords))\n",
                "    \n",
                "    def calculate_bbox(self, polygon: List[List[int]]) -> Tuple[int, int, int, int]:\n",
                "        \"\"\"Calcule la bounding box d'un polygone.\"\"\"\n",
                "        x_coords = [p[0] for p in polygon]\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        return min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
                "    \n",
                "    def similarity_score(self, str1: str, str2: str) -> float:\n",
                "        \"\"\"Calcule un score de similarité entre deux chaînes.\"\"\"\n",
                "        return SequenceMatcher(None, str1.upper(), str2.upper()).ratio()\n",
                "    \n",
                "    def find_label_indices(self, texts: List[str]) -> Dict[str, List[int]]:\n",
                "        \"\"\"Identifie les indices des labels dans le texte OCR.\"\"\"\n",
                "        label_indices = {field: [] for field in self.label_mappings}\n",
                "        \n",
                "        for idx, text in enumerate(texts):\n",
                "            text_upper = text.upper().strip()\n",
                "            \n",
                "            for field, patterns in self.label_mappings.items():\n",
                "                for pattern in patterns:\n",
                "                    # Recherche exacte ou avec similarité élevée\n",
                "                    if pattern in text_upper or self.similarity_score(text_upper, pattern) > 0.85:\n",
                "                        label_indices[field].append(idx)\n",
                "                        break\n",
                "        \n",
                "        return label_indices\n",
                "    \n",
                "    def find_value_by_proximity(self, label_idx: int, polygons: List, texts: List[str], \n",
                "                               scores: List[float], field_type: str) -> Optional[str]:\n",
                "        \"\"\"Trouve la valeur associée à un label par proximité spatiale.\"\"\"\n",
                "        if label_idx >= len(polygons):\n",
                "            return None\n",
                "            \n",
                "        label_center = self.calculate_center(polygons[label_idx])\n",
                "        label_bbox = self.calculate_bbox(polygons[label_idx])\n",
                "        label_height = label_bbox[3] - label_bbox[1]\n",
                "        label_width = label_bbox[2] - label_bbox[0]\n",
                "        \n",
                "        candidates = []\n",
                "        \n",
                "        for idx, (polygon, text, score) in enumerate(zip(polygons, texts, scores)):\n",
                "            if idx == label_idx or not text.strip():\n",
                "                continue\n",
                "                \n",
                "            value_center = self.calculate_center(polygon)\n",
                "            value_bbox = self.calculate_bbox(polygon)\n",
                "            \n",
                "            # Calcul des distances\n",
                "            dx = value_center[0] - label_center[0]\n",
                "            dy = value_center[1] - label_center[1]\n",
                "            \n",
                "            # Zones de recherche prioritaires\n",
                "            is_right = dx > 0 and abs(dy) < label_height * 1.5\n",
                "            is_below = dy > 0 and dy < label_height * 3 and abs(dx) < label_width\n",
                "            \n",
                "            if is_right or is_below:\n",
                "                # Score basé sur la proximité et la confiance OCR\n",
                "                distance = (dx**2 + dy**2) ** 0.5\n",
                "                proximity_score = 1 / (1 + distance / 100)  # Normalisation\n",
                "                combined_score = proximity_score * score\n",
                "                \n",
                "                # Priorité aux éléments à droite sur la même ligne\n",
                "                if is_right and abs(dy) < label_height * 0.5:\n",
                "                    combined_score *= 1.5\n",
                "                \n",
                "                candidates.append({\n",
                "                    'text': text,\n",
                "                    'score': combined_score,\n",
                "                    'distance': distance,\n",
                "                    'position': 'right' if is_right else 'below'\n",
                "                })\n",
                "        \n",
                "        if not candidates:\n",
                "            return None\n",
                "            \n",
                "        # Sélection du meilleur candidat avec validation\n",
                "        candidates.sort(key=lambda x: x['score'], reverse=True)\n",
                "        \n",
                "        for candidate in candidates:\n",
                "            value = candidate['text'].strip()\n",
                "            \n",
                "            # Validation selon le type de champ\n",
                "            if self.validate_field(value, field_type):\n",
                "                return value\n",
                "                \n",
                "        return candidates[0]['text'] if candidates else None\n",
                "    \n",
                "    def validate_field(self, value: str, field_type: str) -> bool:\n",
                "        \"\"\"Valide une valeur selon le type de champ.\"\"\"\n",
                "        if not value:\n",
                "            return False\n",
                "            \n",
                "        if field_type == 'date_naissance':\n",
                "            # Format JJ.MM.AAAA ou JJ/MM/AAAA\n",
                "            date_pattern = r'^\\d{1,2}[./]\\d{1,2}[./]\\d{4}$'\n",
                "            return bool(re.match(date_pattern, value))\n",
                "            \n",
                "        elif field_type == 'sexe':\n",
                "            return value.upper() in ['M', 'F', 'MASCULIN', 'FEMININ', 'MALE', 'FEMALE']\n",
                "            \n",
                "        elif field_type == 'taille':\n",
                "            # Format X,XX ou X.XX\n",
                "            height_pattern = r'^[12][,.]?\\d{2}$'\n",
                "            if re.match(height_pattern, value):\n",
                "                # Vérifier que la valeur est raisonnable\n",
                "                height_val = float(value.replace(',', '.'))\n",
                "                return 1.0 <= height_val <= 2.5\n",
                "            return False\n",
                "            \n",
                "        elif field_type in ['nom', 'prenom']:\n",
                "            # Au moins 2 caractères alphabétiques\n",
                "            return len(value) >= 2 and any(c.isalpha() for c in value)\n",
                "            \n",
                "        elif field_type == 'lieu_naissance':\n",
                "            # Au moins 2 caractères\n",
                "            return len(value) >= 2\n",
                "            \n",
                "        return True\n",
                "    \n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, Optional[str]]:\n",
                "        \"\"\"Extrait les informations de la CNI.\"\"\"\n",
                "        texts = ocr_data['rec_texts']\n",
                "        polygons = ocr_data['rec_polys']\n",
                "        scores = ocr_data['rec_scores']\n",
                "        \n",
                "        # Trouver les indices des labels\n",
                "        label_indices = self.find_label_indices(texts)\n",
                "        \n",
                "        # Extraire les valeurs\n",
                "        extracted = {}\n",
                "        \n",
                "        for field, indices in label_indices.items():\n",
                "            if indices:\n",
                "                # Prendre le label avec le meilleur score OCR\n",
                "                best_idx = max(indices, key=lambda i: scores[i] if i < len(scores) else 0)\n",
                "                value = self.find_value_by_proximity(best_idx, polygons, texts, scores, field)\n",
                "                extracted[field] = value\n",
                "            else:\n",
                "                extracted[field] = None\n",
                "        \n",
                "        # Post-traitement\n",
                "        extracted = self.post_process(extracted, texts, scores)\n",
                "        \n",
                "        return extracted\n",
                "    \n",
                "    def post_process(self, extracted: Dict, texts: List[str], scores: List[float]) -> Dict:\n",
                "        \"\"\"Post-traitement pour améliorer les résultats.\"\"\"\n",
                "        # Si le sexe n'est pas trouvé, chercher 'M' ou 'F' isolé avec bon score\n",
                "        if not extracted.get('sexe'):\n",
                "            for text, score in zip(texts, scores):\n",
                "                if text.strip() in ['M', 'F'] and score > 0.9:\n",
                "                    extracted['sexe'] = text.strip()\n",
                "                    break\n",
                "        \n",
                "        # Nettoyer la taille\n",
                "        if extracted.get('taille'):\n",
                "            taille = extracted['taille']\n",
                "            # Assurer le format X,XX\n",
                "            taille = taille.replace('.', ',')\n",
                "            if ',' not in taille and len(taille) == 3:\n",
                "                taille = taille[0] + ',' + taille[1:]\n",
                "            extracted['taille'] = taille\n",
                "        \n",
                "        return extracted\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    # Charger les données OCR fournies\n",
                "    ocr_data = json.loads(open(\"ocr_outputs\\ID_Card_Kengali_Fegue_Pacome_1_c48073d1_0\\ID_Card_Kengali_Fegue_Pacome_1_res.json\").read())\n",
                "    \n",
                "    extractor = CNIExtractorKeywordProximity()\n",
                "    results = extractor.extract(ocr_data)\n",
                "    \n",
                "    print(\"Informations extraites:\")\n",
                "    for field, value in results.items():\n",
                "        print(f\"{field}: {value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "5e7e7b73",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Résultats extraits:\n",
                        "date_naissance: 29.06.2004\n",
                        "sexe: M\n",
                        "taille: 1,75\n",
                        "nom: REPUBLIQUE DU CAMEROUN\n",
                        "prenom: REPUBLIC OF CAMEROON\n",
                        "lieu_naissance: CARTENATIONALE DIDENTITE\n",
                        "\n",
                        "Score de confiance: 100.00%\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from typing import Dict, List, Optional, Tuple\n",
                "from datetime import datetime\n",
                "\n",
                "class CNIExtractorRegexHeuristics:\n",
                "    def __init__(self):\n",
                "        # Patterns de détection\n",
                "        self.patterns = {\n",
                "            'date': re.compile(r'\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b'),\n",
                "            'sexe': re.compile(r'^[MF]$'),\n",
                "            'taille': re.compile(r'\\b([12])[,.]?(\\d{2})\\b'),\n",
                "            'nom_prenom': re.compile(r'^[A-Z][A-Z\\s\\-]{2,}$'),\n",
                "            'lieu': re.compile(r'^[A-Z][A-Z\\s\\-]{2,}$')\n",
                "        }\n",
                "        \n",
                "        # Mots à exclure (labels OCR)\n",
                "        self.exclude_words = {\n",
                "            'NOM', 'SURNAME', 'PRENOMS', 'PRENOM', 'GIVEN', 'NAMES',\n",
                "            'DATE', 'NAISSANCE', 'BIRTH', 'LIEU', 'PLACE', 'SEXE', \n",
                "            'SEX', 'TAILLE', 'HEIGHT', 'PROFESSION', 'OCCUPATION',\n",
                "            'SIGNATURE', 'REPUBLIQUE', 'CAMEROUN', 'REPUBLIC', 'CAMEROON',\n",
                "            'NATIONALE', 'NATIONAL', 'IDENTITE', 'IDENTITY', 'CARTE', 'CARD',\n",
                "            'DE', 'OF', 'DU'\n",
                "        }\n",
                "    \n",
                "    def calculate_position_score(self, polygon: List[List[int]], \n",
                "                                image_height: int = 1640) -> float:\n",
                "        \"\"\"Calcule un score basé sur la position verticale dans l'image.\"\"\"\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        y_mean = sum(y_coords) / len(y_coords)\n",
                "        # Score plus élevé pour les éléments en haut du document\n",
                "        return 1.0 - (y_mean / image_height)\n",
                "    \n",
                "    def extract_date(self, texts: List[str], scores: List[float], \n",
                "                    polygons: List) -> Tuple[Optional[str], Optional[int]]:\n",
                "        \"\"\"Extrait la date de naissance.\"\"\"\n",
                "        current_year = datetime.now().year\n",
                "        candidates = []\n",
                "        \n",
                "        for idx, (text, score) in enumerate(zip(texts, scores)):\n",
                "            match = self.patterns['date'].search(text)\n",
                "            if match:\n",
                "                jour, mois, annee = match.groups()\n",
                "                annee = int(annee)\n",
                "                \n",
                "                # Validation de l'année\n",
                "                if 1900 <= annee <= current_year - 16:\n",
                "                    position_score = self.calculate_position_score(polygons[idx])\n",
                "                    combined_score = score * position_score\n",
                "                    candidates.append({\n",
                "                        'value': match.group(),\n",
                "                        'index': idx,\n",
                "                        'score': combined_score,\n",
                "                        'year': annee\n",
                "                    })\n",
                "        \n",
                "        if candidates:\n",
                "            # Préférer les dates avec années raisonnables (20-80 ans)\n",
                "            best = max(candidates, key=lambda x: x['score'])\n",
                "            return best['value'], best['index']\n",
                "        \n",
                "        return None, None\n",
                "    \n",
                "    def extract_sexe(self, texts: List[str], scores: List[float]) -> Optional[str]:\n",
                "        \"\"\"Extrait le sexe.\"\"\"\n",
                "        for text, score in zip(texts, scores):\n",
                "            text = text.strip()\n",
                "            if self.patterns['sexe'].match(text) and score > 0.9:\n",
                "                return text\n",
                "        return None\n",
                "    \n",
                "    def extract_taille(self, texts: List[str], scores: List[float]) -> Optional[str]:\n",
                "        \"\"\"Extrait la taille.\"\"\"\n",
                "        for text, score in zip(texts, scores):\n",
                "            match = self.patterns['taille'].search(text)\n",
                "            if match and score > 0.8:\n",
                "                metres, centimetres = match.groups()\n",
                "                # Reconstituer au format X,XX\n",
                "                taille_str = f\"{metres},{centimetres}\"\n",
                "                taille_val = float(f\"{metres}.{centimetres}\")\n",
                "                \n",
                "                # Validation de la valeur\n",
                "                if 1.0 <= taille_val <= 2.5:\n",
                "                    return taille_str\n",
                "        \n",
                "        return None\n",
                "    \n",
                "    def is_valid_name(self, text: str) -> bool:\n",
                "        \"\"\"Vérifie si le texte peut être un nom/prénom.\"\"\"\n",
                "        text = text.strip()\n",
                "        \n",
                "        # Exclure les mots réservés\n",
                "        if text.upper() in self.exclude_words:\n",
                "            return False\n",
                "        \n",
                "        # Doit contenir au moins 3 caractères alphabétiques\n",
                "        if len(text) < 3:\n",
                "            return False\n",
                "        \n",
                "        # Majoritairement alphabétique\n",
                "        alpha_count = sum(1 for c in text if c.isalpha())\n",
                "        if alpha_count < len(text) * 0.8:\n",
                "            return False\n",
                "        \n",
                "        # Pattern de nom\n",
                "        return bool(self.patterns['nom_prenom'].match(text))\n",
                "    \n",
                "    def extract_nom_prenom(self, texts: List[str], scores: List[float], \n",
                "                          polygons: List, date_idx: Optional[int]) -> Dict[str, Optional[str]]:\n",
                "        \"\"\"Extrait le nom et le prénom basé sur les heuristiques de position.\"\"\"\n",
                "        candidates = []\n",
                "        \n",
                "        # Calculer la hauteur maximale pour le tiers supérieur\n",
                "        all_y = [p[1] for poly in polygons for p in poly]\n",
                "        max_y = max(all_y) if all_y else 1640\n",
                "        threshold_y = max_y * 0.4  # Zone supérieure\n",
                "        \n",
                "        for idx, (text, score, polygon) in enumerate(zip(texts, scores, polygons)):\n",
                "            if not self.is_valid_name(text):\n",
                "                continue\n",
                "            \n",
                "            y_coords = [p[1] for p in polygon]\n",
                "            y_mean = sum(y_coords) / len(y_coords)\n",
                "            \n",
                "            # Doit être dans la partie supérieure et avant la date\n",
                "            if y_mean < threshold_y and (date_idx is None or idx < date_idx):\n",
                "                if score > 0.85:\n",
                "                    candidates.append({\n",
                "                        'text': text.strip(),\n",
                "                        'index': idx,\n",
                "                        'y_position': y_mean,\n",
                "                        'score': score\n",
                "                    })\n",
                "        \n",
                "        # Trier par position verticale\n",
                "        candidates.sort(key=lambda x: x['y_position'])\n",
                "        \n",
                "        result = {'nom': None, 'prenom': None}\n",
                "        \n",
                "        # Les deux premiers candidats valides sont probablement nom et prénom\n",
                "        if len(candidates) >= 1:\n",
                "            result['nom'] = candidates[0]['text']\n",
                "        if len(candidates) >= 2:\n",
                "            result['prenom'] = candidates[1]['text']\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    def extract_lieu_naissance(self, texts: List[str], scores: List[float], \n",
                "                              polygons: List, date_idx: Optional[int]) -> Optional[str]:\n",
                "        \"\"\"Extrait le lieu de naissance.\"\"\"\n",
                "        if date_idx is None:\n",
                "            return None\n",
                "        \n",
                "        # Chercher dans une fenêtre après la date\n",
                "        window_start = date_idx + 1\n",
                "        window_end = min(date_idx + 5, len(texts))\n",
                "        \n",
                "        for idx in range(window_start, window_end):\n",
                "            text = texts[idx].strip()\n",
                "            score = scores[idx]\n",
                "            \n",
                "            # Vérifier que c'est un nom de lieu valide\n",
                "            if (self.is_valid_name(text) and \n",
                "                text.upper() not in self.exclude_words and\n",
                "                score > 0.85):\n",
                "                return text\n",
                "        \n",
                "        return None\n",
                "    \n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, Optional[str]]:\n",
                "        \"\"\"Extraction principale utilisant regex et heuristiques.\"\"\"\n",
                "        texts = ocr_data['rec_texts']\n",
                "        scores = ocr_data['rec_scores']\n",
                "        polygons = ocr_data['rec_polys']\n",
                "        \n",
                "        # Extraction séquentielle\n",
                "        results = {}\n",
                "        \n",
                "        # 1. Date (point d'ancrage important)\n",
                "        date_value, date_idx = self.extract_date(texts, scores, polygons)\n",
                "        results['date_naissance'] = date_value\n",
                "        \n",
                "        # 2. Sexe (indépendant)\n",
                "        results['sexe'] = self.extract_sexe(texts, scores)\n",
                "        \n",
                "        # 3. Taille (indépendant)\n",
                "        results['taille'] = self.extract_taille(texts, scores)\n",
                "        \n",
                "        # 4. Nom et Prénom (basé sur position)\n",
                "        nom_prenom = self.extract_nom_prenom(texts, scores, polygons, date_idx)\n",
                "        results.update(nom_prenom)\n",
                "        \n",
                "        # 5. Lieu de naissance (basé sur proximité avec date)\n",
                "        results['lieu_naissance'] = self.extract_lieu_naissance(\n",
                "            texts, scores, polygons, date_idx\n",
                "        )\n",
                "        \n",
                "        # Post-traitement et validation croisée\n",
                "        results = self.post_process(results, texts, scores)\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def post_process(self, results: Dict, texts: List[str], \n",
                "                    scores: List[float]) -> Dict:\n",
                "        \"\"\"Post-traitement et corrections finales.\"\"\"\n",
                "        \n",
                "        # Si pas de lieu trouvé, chercher DSCHANG ou autres villes connues\n",
                "        if not results.get('lieu_naissance'):\n",
                "            villes_cameroun = ['DOUALA', 'YAOUNDE', 'DSCHANG', 'BAMENDA', \n",
                "                              'BAFOUSSAM', 'GAROUA', 'MAROUA', 'NGAOUNDERE']\n",
                "            for text in texts:\n",
                "                text_upper = text.strip().upper()\n",
                "                if text_upper in villes_cameroun:\n",
                "                    results['lieu_naissance'] = text\n",
                "                    break\n",
                "        \n",
                "        # Validation croisée nom/prénom\n",
                "        if results.get('nom') and results.get('prenom'):\n",
                "            # Si le \"nom\" contient des mots du label, inverser\n",
                "            if any(word in results['nom'].upper() for word in ['PRENOM', 'GIVEN']):\n",
                "                results['nom'], results['prenom'] = results['prenom'], results['nom']\n",
                "        \n",
                "        # Nettoyer les valeurs\n",
                "        for key in results:\n",
                "            if results[key]:\n",
                "                results[key] = results[key].strip()\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def confidence_score(self, results: Dict) -> float:\n",
                "        \"\"\"Calcule un score de confiance global.\"\"\"\n",
                "        filled_fields = sum(1 for v in results.values() if v is not None)\n",
                "        total_fields = len(results)\n",
                "        return filled_fields / total_fields if total_fields > 0 else 0.0\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    import json\n",
                "    \n",
                "    # Simuler le chargement des données\n",
                "    ocr_data = json.loads(open(\"ocr_outputs\\ID_Card_Kengali_Fegue_Pacome_1_c48073d1_0\\ID_Card_Kengali_Fegue_Pacome_1_res.json\").read())\n",
                "\n",
                "    \n",
                "    extractor = CNIExtractorRegexHeuristics()\n",
                "    results = extractor.extract(ocr_data)\n",
                "    confidence = extractor.confidence_score(results)\n",
                "    \n",
                "    print(\"Résultats extraits:\")\n",
                "    for field, value in results.items():\n",
                "        print(f\"{field}: {value}\")\n",
                "    print(f\"\\nScore de confiance: {confidence:.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "17a54928",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Résultats extraits par template matching:\n",
                        "nom: KENGALIFEGUE (confiance: 90%)\n",
                        "prenom: KENGALIFEGUE (confiance: 90%)\n",
                        "date_naissance: 29.06.2004 (confiance: 90%)\n",
                        "lieu_naissance: PACOME (confiance: 90%)\n",
                        "sexe: M (confiance: 90%)\n",
                        "taille: 1,75 (confiance: 90%)\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "import re\n",
                "\n",
                "class CNIExtractorTemplateMatching:\n",
                "    def __init__(self):\n",
                "        # Template de référence : positions relatives des champs (x, y, largeur, hauteur)\n",
                "        # Valeurs en pourcentage de la taille totale du document\n",
                "        self.template_zones = {\n",
                "            'nom': {\n",
                "                'x': 0.35, 'y': 0.15, 'width': 0.30, 'height': 0.05,\n",
                "                'anchors': ['NOM', 'SURNAME']\n",
                "            },\n",
                "            'prenom': {\n",
                "                'x': 0.35, 'y': 0.20, 'width': 0.30, 'height': 0.05,\n",
                "                'anchors': ['PRENOMS', 'GIVEN NAMES']\n",
                "            },\n",
                "            'date_naissance': {\n",
                "                'x': 0.35, 'y': 0.30, 'width': 0.25, 'height': 0.05,\n",
                "                'anchors': ['DATE DE NAISSANCE', 'DATE OF BIRTH']\n",
                "            },\n",
                "            'lieu_naissance': {\n",
                "                'x': 0.35, 'y': 0.35, 'width': 0.30, 'height': 0.05,\n",
                "                'anchors': ['LIEU DE NAISSANCE', 'PLACE OF BIRTH']\n",
                "            },\n",
                "            'sexe': {\n",
                "                'x': 0.35, 'y': 0.40, 'width': 0.10, 'height': 0.05,\n",
                "                'anchors': ['SEXE', 'SEX']\n",
                "            },\n",
                "            'taille': {\n",
                "                'x': 0.50, 'y': 0.40, 'width': 0.15, 'height': 0.05,\n",
                "                'anchors': ['TAILLE', 'HEIGHT']\n",
                "            }\n",
                "        }\n",
                "        \n",
                "        # Points d'ancrage pour l'alignement\n",
                "        self.alignment_anchors = [\n",
                "            'REPUBLIQUE DU CAMEROUN',\n",
                "            'REPUBLIC OF CAMEROON',\n",
                "            'CARTE NATIONALE',\n",
                "            'NATIONAL IDENTITY CARD'\n",
                "        ]\n",
                "    \n",
                "    def find_document_bounds(self, polygons: List) -> Tuple[int, int, int, int]:\n",
                "        \"\"\"Trouve les limites du document dans l'image.\"\"\"\n",
                "        all_points = [point for poly in polygons for point in poly]\n",
                "        if not all_points:\n",
                "            return 0, 0, 2000, 1600\n",
                "        \n",
                "        x_coords = [p[0] for p in all_points]\n",
                "        y_coords = [p[1] for p in all_points]\n",
                "        \n",
                "        return min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
                "    \n",
                "    def detect_anchor_points(self, texts: List[str], polygons: List) -> List[Dict]:\n",
                "        \"\"\"Détecte les points d'ancrage pour l'alignement.\"\"\"\n",
                "        anchors = []\n",
                "        \n",
                "        for idx, text in enumerate(texts):\n",
                "            text_upper = text.strip().upper()\n",
                "            \n",
                "            # Vérifier si c'est un point d'ancrage\n",
                "            for anchor_text in self.alignment_anchors:\n",
                "                if anchor_text in text_upper or self.similarity(text_upper, anchor_text) > 0.8:\n",
                "                    # Calculer le centre du polygone\n",
                "                    polygon = polygons[idx]\n",
                "                    center_x = sum(p[0] for p in polygon) / len(polygon)\n",
                "                    center_y = sum(p[1] for p in polygon) / len(polygon)\n",
                "                    \n",
                "                    anchors.append({\n",
                "                        'text': text,\n",
                "                        'expected': anchor_text,\n",
                "                        'center': (center_x, center_y),\n",
                "                        'polygon': polygon,\n",
                "                        'index': idx\n",
                "                    })\n",
                "                    break\n",
                "        \n",
                "        return anchors\n",
                "    \n",
                "    def similarity(self, str1: str, str2: str) -> float:\n",
                "        \"\"\"Calcule la similarité entre deux chaînes.\"\"\"\n",
                "        set1 = set(str1.split())\n",
                "        set2 = set(str2.split())\n",
                "        intersection = set1.intersection(set2)\n",
                "        union = set1.union(set2)\n",
                "        return len(intersection) / len(union) if union else 0.0\n",
                "    \n",
                "    def estimate_transformation(self, anchors: List[Dict], \n",
                "                               doc_bounds: Tuple[int, int, int, int]) -> Dict:\n",
                "        \"\"\"Estime la transformation géométrique du document.\"\"\"\n",
                "        min_x, min_y, max_x, max_y = doc_bounds\n",
                "        doc_width = max_x - min_x\n",
                "        doc_height = max_y - min_y\n",
                "        \n",
                "        # Paramètres de transformation par défaut\n",
                "        transform = {\n",
                "            'scale_x': 1.0,\n",
                "            'scale_y': 1.0,\n",
                "            'rotation': 0.0,\n",
                "            'offset_x': min_x,\n",
                "            'offset_y': min_y,\n",
                "            'width': doc_width,\n",
                "            'height': doc_height\n",
                "        }\n",
                "        \n",
                "        # Si on a au moins 2 points d'ancrage, estimer la rotation\n",
                "        if len(anchors) >= 2:\n",
                "            # Prendre les deux premiers points d'ancrage\n",
                "            p1 = anchors[0]['center']\n",
                "            p2 = anchors[1]['center']\n",
                "            \n",
                "            # Calculer l'angle de rotation\n",
                "            dx = p2[0] - p1[0]\n",
                "            dy = p2[1] - p1[1]\n",
                "            \n",
                "            # Angle en radians (assumant que le document devrait être horizontal)\n",
                "            angle = np.arctan2(dy, dx)\n",
                "            \n",
                "            # Normaliser l'angle (proche de 0 ou 90 degrés)\n",
                "            if abs(angle) < np.pi/4:\n",
                "                transform['rotation'] = angle\n",
                "            elif abs(angle - np.pi/2) < np.pi/4:\n",
                "                transform['rotation'] = angle - np.pi/2\n",
                "        \n",
                "        return transform\n",
                "    \n",
                "    def transform_point(self, x: float, y: float, transform: Dict, inverse: bool = False) -> Tuple[float, float]:\n",
                "        \"\"\"Applique ou inverse une transformation à un point.\"\"\"\n",
                "        if inverse:\n",
                "            # Transformation inverse : template -> image\n",
                "            # Dénormaliser\n",
                "            x_abs = x * transform['width'] + transform['offset_x']\n",
                "            y_abs = y * transform['height'] + transform['offset_y']\n",
                "            \n",
                "            # Appliquer la rotation inverse si nécessaire\n",
                "            if transform['rotation'] != 0:\n",
                "                cos_r = np.cos(-transform['rotation'])\n",
                "                sin_r = np.sin(-transform['rotation'])\n",
                "                cx = transform['offset_x'] + transform['width'] / 2\n",
                "                cy = transform['offset_y'] + transform['height'] / 2\n",
                "                \n",
                "                x_rot = cos_r * (x_abs - cx) - sin_r * (y_abs - cy) + cx\n",
                "                y_rot = sin_r * (x_abs - cx) + cos_r * (y_abs - cy) + cy\n",
                "                return x_rot, y_rot\n",
                "            \n",
                "            return x_abs, y_abs\n",
                "        else:\n",
                "            # Transformation directe : image -> template\n",
                "            # Appliquer la rotation si nécessaire\n",
                "            if transform['rotation'] != 0:\n",
                "                cos_r = np.cos(transform['rotation'])\n",
                "                sin_r = np.sin(transform['rotation'])\n",
                "                cx = transform['offset_x'] + transform['width'] / 2\n",
                "                cy = transform['offset_y'] + transform['height'] / 2\n",
                "                \n",
                "                x_rot = cos_r * (x - cx) - sin_r * (y - cy) + cx\n",
                "                y_rot = sin_r * (x - cx) + cos_r * (y - cy) + cy\n",
                "                x, y = x_rot, y_rot\n",
                "            \n",
                "            # Normaliser\n",
                "            x_norm = (x - transform['offset_x']) / transform['width']\n",
                "            y_norm = (y - transform['offset_y']) / transform['height']\n",
                "            \n",
                "            return x_norm, y_norm\n",
                "    \n",
                "    def extract_from_zone(self, zone_def: Dict, texts: List[str], \n",
                "                         polygons: List, scores: List[float], \n",
                "                         transform: Dict) -> Optional[str]:\n",
                "        \"\"\"Extrait le texte d'une zone spécifique après transformation.\"\"\"\n",
                "        # Convertir la zone template en coordonnées image\n",
                "        zone_x1, zone_y1 = self.transform_point(\n",
                "            zone_def['x'], zone_def['y'], transform, inverse=True\n",
                "        )\n",
                "        zone_x2, zone_y2 = self.transform_point(\n",
                "            zone_def['x'] + zone_def['width'], \n",
                "            zone_def['y'] + zone_def['height'], \n",
                "            transform, inverse=True\n",
                "        )\n",
                "        \n",
                "        # Assurer l'ordre correct des coordonnées\n",
                "        x1, x2 = min(zone_x1, zone_x2), max(zone_x1, zone_x2)\n",
                "        y1, y2 = min(zone_y1, zone_y2), max(zone_y1, zone_y2)\n",
                "        \n",
                "        # Étendre légèrement la zone pour la tolérance\n",
                "        margin_x = (x2 - x1) * 0.2\n",
                "        margin_y = (y2 - y1) * 0.2\n",
                "        x1 -= margin_x\n",
                "        x2 += margin_x\n",
                "        y1 -= margin_y\n",
                "        y2 += margin_y\n",
                "        \n",
                "        candidates = []\n",
                "        \n",
                "        for idx, (text, polygon, score) in enumerate(zip(texts, polygons, scores)):\n",
                "            if not text.strip():\n",
                "                continue\n",
                "            \n",
                "            # Calculer le centre du polygone\n",
                "            center_x = sum(p[0] for p in polygon) / len(polygon)\n",
                "            center_y = sum(p[1] for p in polygon) / len(polygon)\n",
                "            \n",
                "            # Vérifier si le centre est dans la zone\n",
                "            if x1 <= center_x <= x2 and y1 <= center_y <= y2:\n",
                "                # Calculer le score basé sur la proximité au centre de la zone\n",
                "                zone_center_x = (x1 + x2) / 2\n",
                "                zone_center_y = (y1 + y2) / 2\n",
                "                distance = ((center_x - zone_center_x)**2 + (center_y - zone_center_y)**2) ** 0.5\n",
                "                \n",
                "                # Normaliser la distance\n",
                "                max_distance = ((x2 - x1)**2 + (y2 - y1)**2) ** 0.5\n",
                "                proximity_score = 1 - (distance / max_distance) if max_distance > 0 else 1\n",
                "                \n",
                "                # Exclure les mots d'ancrage\n",
                "                is_anchor = False\n",
                "                for anchor in zone_def.get('anchors', []):\n",
                "                    if anchor.upper() in text.upper():\n",
                "                        is_anchor = True\n",
                "                        break\n",
                "                \n",
                "                if not is_anchor:\n",
                "                    candidates.append({\n",
                "                        'text': text.strip(),\n",
                "                        'score': score * proximity_score,\n",
                "                        'ocr_score': score,\n",
                "                        'proximity': proximity_score\n",
                "                    })\n",
                "        \n",
                "        # Sélectionner le meilleur candidat\n",
                "        if candidates:\n",
                "            # Trier par score combiné\n",
                "            candidates.sort(key=lambda x: x['score'], reverse=True)\n",
                "            return candidates[0]['text']\n",
                "        \n",
                "        return None\n",
                "    \n",
                "    def validate_extraction(self, field: str, value: Optional[str]) -> bool:\n",
                "        \"\"\"Valide la valeur extraite selon le type de champ.\"\"\"\n",
                "        if not value:\n",
                "            return False\n",
                "        \n",
                "        if field == 'date_naissance':\n",
                "            # Format date JJ.MM.AAAA ou JJ/MM/AAAA\n",
                "            pattern = r'^\\d{1,2}[./]\\d{1,2}[./]\\d{4}'\n",
                "            return bool(re.match(pattern, value))\n",
                "        \n",
                "        elif field == 'sexe':\n",
                "            return value.upper() in ['M', 'F']\n",
                "        \n",
                "        elif field == 'taille':\n",
                "            # Format taille X,XX ou X.XX\n",
                "            pattern = r'^[12][,.]?\\d{2}'\n",
                "            return bool(re.match(pattern, value))\n",
                "        \n",
                "        elif field in ['nom', 'prenom', 'lieu_naissance']:\n",
                "            # Au moins 2 caractères alphabétiques\n",
                "            return len(value) >= 2 and any(c.isalpha() for c in value)\n",
                "        \n",
                "        return True\n",
                "    \n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, Optional[str]]:\n",
                "        \"\"\"Extraction principale par template matching.\"\"\"\n",
                "        texts = ocr_data['rec_texts']\n",
                "        polygons = ocr_data['rec_polys']\n",
                "        scores = ocr_data['rec_scores']\n",
                "        \n",
                "        # 1. Trouver les limites du document\n",
                "        doc_bounds = self.find_document_bounds(polygons)\n",
                "        \n",
                "        # 2. Détecter les points d'ancrage\n",
                "        anchors = self.detect_anchor_points(texts, polygons)\n",
                "        \n",
                "        # 3. Estimer la transformation\n",
                "        transform = self.estimate_transformation(anchors, doc_bounds)\n",
                "        \n",
                "        # 4. Extraire les champs depuis les zones du template\n",
                "        results = {}\n",
                "        \n",
                "        for field, zone_def in self.template_zones.items():\n",
                "            value = self.extract_from_zone(zone_def, texts, polygons, scores, transform)\n",
                "            \n",
                "            # Valider l'extraction\n",
                "            if self.validate_extraction(field, value):\n",
                "                results[field] = value\n",
                "            else:\n",
                "                results[field] = None\n",
                "        \n",
                "        # 5. Post-traitement et récupération des champs manquants\n",
                "        results = self.fallback_extraction(results, texts, scores)\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def fallback_extraction(self, results: Dict, texts: List[str], \n",
                "                           scores: List[float]) -> Dict:\n",
                "        \"\"\"Méthode de secours pour les champs non trouvés.\"\"\"\n",
                "        \n",
                "        # Si la date n'est pas trouvée, chercher par pattern\n",
                "        if not results.get('date_naissance'):\n",
                "            date_pattern = re.compile(r'\\d{1,2}[./]\\d{1,2}[./]\\d{4}')\n",
                "            for text, score in zip(texts, scores):\n",
                "                if date_pattern.match(text) and score > 0.9:\n",
                "                    results['date_naissance'] = text\n",
                "                    break\n",
                "        \n",
                "        # Si le sexe n'est pas trouvé\n",
                "        if not results.get('sexe'):\n",
                "            for text, score in zip(texts, scores):\n",
                "                if text.strip() in ['M', 'F'] and score > 0.9:\n",
                "                    results['sexe'] = text.strip()\n",
                "                    break\n",
                "        \n",
                "        # Si la taille n'est pas trouvée\n",
                "        if not results.get('taille'):\n",
                "            taille_pattern = re.compile(r'[12][,.]?\\d{2}')\n",
                "            for text, score in zip(texts, scores):\n",
                "                match = taille_pattern.match(text)\n",
                "                if match and score > 0.9:\n",
                "                    # Formater correctement\n",
                "                    taille = text.replace('.', ',')\n",
                "                    if ',' not in taille and len(taille) == 3:\n",
                "                        taille = taille[0] + ',' + taille[1:]\n",
                "                    results['taille'] = taille\n",
                "                    break\n",
                "        \n",
                "        # Nettoyer les valeurs\n",
                "        for key in results:\n",
                "            if results[key]:\n",
                "                results[key] = results[key].strip()\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def compute_confidence(self, results: Dict) -> Dict[str, float]:\n",
                "        \"\"\"Calcule un score de confiance pour chaque champ extrait.\"\"\"\n",
                "        confidence = {}\n",
                "        for field, value in results.items():\n",
                "            if value:\n",
                "                # Confiance basée sur la validation\n",
                "                if self.validate_extraction(field, value):\n",
                "                    confidence[field] = 0.9\n",
                "                else:\n",
                "                    confidence[field] = 0.5\n",
                "            else:\n",
                "                confidence[field] = 0.0\n",
                "        \n",
                "        return confidence\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    import json\n",
                "    \n",
                "    # Charger les données OCR\n",
                "    ocr_data = json.loads(open(\"ocr_outputs\\ID_Card_Kengali_Fegue_Pacome_1_c48073d1_0\\ID_Card_Kengali_Fegue_Pacome_1_res.json\").read())\n",
                "\n",
                "    \n",
                "    extractor = CNIExtractorTemplateMatching()\n",
                "    results = extractor.extract(ocr_data)\n",
                "    confidence = extractor.compute_confidence(results)\n",
                "    \n",
                "    print(\"Résultats extraits par template matching:\")\n",
                "    for field, value in results.items():\n",
                "        conf = confidence.get(field, 0.0)\n",
                "        print(f\"{field}: {value} (confiance: {conf:.0%})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "d57612d1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] ==================================================\n",
                        "[INFO] DÉBUT DE L'EXTRACTION CNI\n",
                        "[INFO] ==================================================\n",
                        "[INFO] === ÉVALUATION DE LA QUALITÉ ===\n",
                        "[INFO] Score moyen: 0.92\n",
                        "[INFO] Éléments de bonne qualité (>0.7): 17/20\n",
                        "[INFO] Éléments valides: 18\n",
                        "[INFO] Peut continuer: True\n",
                        "[INFO] === PRÉPROCESSING ===\n",
                        "[DEBUG]   Filtré (score faible): '1' (score=0.07)\n",
                        "[INFO]   Gardé: 'REPUBLIQUE DU CAMEROUN' (score=0.97)\n",
                        "[INFO]   Gardé: 'REPUBLIC OF CAMEROON' (score=0.96)\n",
                        "[INFO]   Gardé: 'NOM/SURNAME' (score=0.99)\n",
                        "[INFO]   Gardé: 'AKONDACK AITAOY' (score=0.98)\n",
                        "[INFO]   Gardé: 'PRENOMS/GIVEN NAMES' (score=0.98)\n",
                        "[INFO]   Gardé: 'YVETTE MARIAN' (score=0.98)\n",
                        "[DEBUG]   Filtré (score faible): '' (score=0.00)\n",
                        "[INFO]   Gardé: 'DATE DE NAISSANCE/DATE OF BIRTH' (score=0.95)\n",
                        "[INFO]   Gardé: '19.04.1999' (score=0.99)\n",
                        "[INFO]   Gardé: 'LIEU DE NAISSANCE/PLACE OF BIRTH' (score=0.93)\n",
                        "[INFO]   Gardé: 'BAFIA' (score=0.99)\n",
                        "[DEBUG]   Filtré (score faible): '' (score=0.00)\n",
                        "[INFO]   Gardé: 'SEXE/SEX' (score=0.99)\n",
                        "[INFO]   Gardé: 'TAILLE/HEIGHT' (score=0.99)\n",
                        "[INFO]   Gardé: 'F' (score=0.90)\n",
                        "[INFO]   Gardé: '1,73' (score=0.95)\n",
                        "[INFO]   Gardé: 'PROFESSION/OCCUPATION' (score=0.99)\n",
                        "[INFO]   Gardé: 'COIFFEUSE' (score=0.99)\n",
                        "[INFO]   Gardé: 'SIGNATURE' (score=0.99)\n",
                        "[INFO] Éléments après préprocessing: 17/20\n",
                        "[INFO] === EXTRACTION DES CHAMPS À FORMAT FIXE ===\n",
                        "[INFO]   Date trouvée: '19.04.1999' à l'index 7\n",
                        "[INFO]   Sexe trouvé: 'F' à l'index 12\n",
                        "[INFO]   Taille trouvée: '1,73' -> '1,73' à l'index 13\n",
                        "[INFO] Champs fixes extraits: Date=19.04.1999, Sexe=F, Taille=1,73\n",
                        "[INFO] Indices à retirer: [7, 12, 13]\n",
                        "[INFO] \n",
                        "Éléments restants après extraction des champs fixes: 14\n",
                        "[INFO]   - 'REPUBLIQUE DU CAMEROUN' (score=0.97)\n",
                        "[INFO]   - 'REPUBLIC OF CAMEROON' (score=0.96)\n",
                        "[INFO]   - 'NOM/SURNAME' (score=0.99)\n",
                        "[INFO]   - 'AKONDACK AITAOY' (score=0.98)\n",
                        "[INFO]   - 'PRENOMS/GIVEN NAMES' (score=0.98)\n",
                        "[INFO]   - 'YVETTE MARIAN' (score=0.98)\n",
                        "[INFO]   - 'DATE DE NAISSANCE/DATE OF BIRTH' (score=0.95)\n",
                        "[INFO]   - 'LIEU DE NAISSANCE/PLACE OF BIRTH' (score=0.93)\n",
                        "[INFO]   - 'BAFIA' (score=0.99)\n",
                        "[INFO]   - 'SEXE/SEX' (score=0.99)\n",
                        "[INFO]   - 'TAILLE/HEIGHT' (score=0.99)\n",
                        "[INFO]   - 'PROFESSION/OCCUPATION' (score=0.99)\n",
                        "[INFO]   - 'COIFFEUSE' (score=0.99)\n",
                        "[INFO]   - 'SIGNATURE' (score=0.99)\n",
                        "[INFO] === DÉTECTION DES ANCRES ===\n",
                        "[INFO]   Ancre détectée pour 'nom': 'NOM/SURNAME' ~ 'SURNAME' (similarité=0.78)\n",
                        "[INFO]   Ancre détectée pour 'prenom': 'PRENOMS/GIVEN NAMES' ~ 'PRENOMS' (similarité=0.72)\n",
                        "[INFO]   Ancre détectée pour 'lieu_naissance': 'DATE DE NAISSANCE/DATE OF BIRTH' ~ 'LIEU DE NAISSANCE/PLACEOF BIRTH' (similarité=0.81)\n",
                        "[INFO]   Ancre détectée pour 'lieu_naissance': 'LIEU DE NAISSANCE/PLACE OF BIRTH' ~ 'LIEU DE NAISSANCE' (similarité=0.82)\n",
                        "[INFO]   Ancre détectée pour 'profession': 'PROFESSION/OCCUPATION' ~ 'PROFESSION' (similarité=0.79)\n",
                        "[INFO]   nom: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   prenom: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   lieu_naissance: 2 ancre(s) trouvée(s)\n",
                        "[INFO]   profession: 1 ancre(s) trouvée(s)\n",
                        "[INFO] === EXTRACTION DES CHAMPS RESTANTS ===\n",
                        "[INFO] Extraction par ancre pour 'nom':\n",
                        "[INFO]   Recherche de valeur pour 'nom' près de l'ancre à l'index 2\n",
                        "[INFO]     Ancre: 'NOM/SURNAME' au centre (988.0, 335.5)\n",
                        "[DEBUG]     'REPUBLIQUE DU CAMEROUN' identifié comme label (similaire à 'REPUBLIQUE', score=0.78)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIQUE DU CAMEROUN'\n",
                        "[DEBUG]     'REPUBLIC OF CAMEROON' identifié comme label (mots clés: 3/3)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIC OF CAMEROON'\n",
                        "[INFO]     Candidat: 'AKONDACK AITAOY' (droite, distance=95, score=0.50)\n",
                        "[DEBUG]     'PRENOMS/GIVEN NAMES' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PRENOMS/GIVEN NAMES'\n",
                        "[INFO]     Candidat: 'YVETTE MARIAN' (droite, distance=346, score=0.22)\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[DEBUG]     'LIEU DE NAISSANCE/PLACE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'LIEU DE NAISSANCE/PLACE OF BIRTH'\n",
                        "[INFO]     Candidat: 'BAFIA' (dessous, distance=778, score=0.11)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'TAILLE/HEIGHT' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'TAILLE/HEIGHT'\n",
                        "[DEBUG]     'PROFESSION/OCCUPATION' identifié comme label (similaire à 'PROFESSION', score=0.79)\n",
                        "[DEBUG]     Ignoré (label): 'PROFESSION/OCCUPATION'\n",
                        "[INFO]     Candidat: 'COIFFEUSE' (dessous, distance=1128, score=0.08)\n",
                        "[DEBUG]     'SIGNATURE' identifié comme label (similaire à 'SIGNATURE', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE'\n",
                        "[INFO]   Meilleur candidat pour 'nom': 'AKONDACK AITAOY'\n",
                        "[INFO]   'nom' = 'AKONDACK AITAOY' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'prenom':\n",
                        "[INFO]   Recherche de valeur pour 'prenom' près de l'ancre à l'index 4\n",
                        "[INFO]     Ancre: 'PRENOMS/GIVEN NAMES' au centre (1094.5, 605.0)\n",
                        "[DEBUG]     'REPUBLIQUE DU CAMEROUN' identifié comme label (similaire à 'REPUBLIQUE', score=0.78)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIQUE DU CAMEROUN'\n",
                        "[DEBUG]     'REPUBLIC OF CAMEROON' identifié comme label (mots clés: 3/3)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIC OF CAMEROON'\n",
                        "[DEBUG]     'NOM/SURNAME' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'NOM/SURNAME'\n",
                        "[INFO]     Candidat: 'YVETTE MARIAN' (dessous, distance=121, score=0.44)\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[DEBUG]     'LIEU DE NAISSANCE/PLACE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'LIEU DE NAISSANCE/PLACE OF BIRTH'\n",
                        "[INFO]     Candidat: 'BAFIA' (dessous, distance=544, score=0.15)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'TAILLE/HEIGHT' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'TAILLE/HEIGHT'\n",
                        "[DEBUG]     'PROFESSION/OCCUPATION' identifié comme label (similaire à 'PROFESSION', score=0.79)\n",
                        "[DEBUG]     Ignoré (label): 'PROFESSION/OCCUPATION'\n",
                        "[INFO]     Candidat: 'COIFFEUSE' (dessous, distance=868, score=0.10)\n",
                        "[DEBUG]     'SIGNATURE' identifié comme label (similaire à 'SIGNATURE', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE'\n",
                        "[INFO]   Meilleur candidat pour 'prenom': 'YVETTE MARIAN'\n",
                        "[INFO]   'prenom' = 'YVETTE MARIAN' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'lieu_naissance':\n",
                        "[INFO]   Recherche de valeur pour 'lieu_naissance' près de l'ancre à l'index 7\n",
                        "[INFO]     Ancre: 'LIEU DE NAISSANCE/PLACE OF BIRTH' au centre (1260.75, 1041.75)\n",
                        "[DEBUG]     'REPUBLIQUE DU CAMEROUN' identifié comme label (similaire à 'REPUBLIQUE', score=0.78)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIQUE DU CAMEROUN'\n",
                        "[DEBUG]     'REPUBLIC OF CAMEROON' identifié comme label (mots clés: 3/3)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIC OF CAMEROON'\n",
                        "[DEBUG]     'NOM/SURNAME' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'NOM/SURNAME'\n",
                        "[DEBUG]     'PRENOMS/GIVEN NAMES' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PRENOMS/GIVEN NAMES'\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[INFO]     Candidat: 'BAFIA' (dessous, distance=382, score=0.21)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'TAILLE/HEIGHT' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'TAILLE/HEIGHT'\n",
                        "[DEBUG]     'PROFESSION/OCCUPATION' identifié comme label (similaire à 'PROFESSION', score=0.79)\n",
                        "[DEBUG]     Ignoré (label): 'PROFESSION/OCCUPATION'\n",
                        "[INFO]     Candidat: 'COIFFEUSE' (dessous, distance=515, score=0.16)\n",
                        "[DEBUG]     'SIGNATURE' identifié comme label (similaire à 'SIGNATURE', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE'\n",
                        "[INFO]   Meilleur candidat pour 'lieu_naissance': 'BAFIA'\n",
                        "[INFO]   'lieu_naissance' = 'BAFIA' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'profession':\n",
                        "[INFO]   Recherche de valeur pour 'profession' près de l'ancre à l'index 11\n",
                        "[INFO]     Ancre: 'PROFESSION/OCCUPATION' au centre (1143.0, 1386.25)\n",
                        "[DEBUG]     'REPUBLIQUE DU CAMEROUN' identifié comme label (similaire à 'REPUBLIQUE', score=0.78)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIQUE DU CAMEROUN'\n",
                        "[DEBUG]     'REPUBLIC OF CAMEROON' identifié comme label (mots clés: 3/3)\n",
                        "[DEBUG]     Ignoré (label): 'REPUBLIC OF CAMEROON'\n",
                        "[DEBUG]     'NOM/SURNAME' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'NOM/SURNAME'\n",
                        "[DEBUG]     'PRENOMS/GIVEN NAMES' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PRENOMS/GIVEN NAMES'\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[DEBUG]     'LIEU DE NAISSANCE/PLACE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'LIEU DE NAISSANCE/PLACE OF BIRTH'\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'TAILLE/HEIGHT' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'TAILLE/HEIGHT'\n",
                        "[INFO]     Candidat: 'COIFFEUSE' (dessous, distance=195, score=0.34)\n",
                        "[DEBUG]     'SIGNATURE' identifié comme label (similaire à 'SIGNATURE', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE'\n",
                        "[INFO]   Meilleur candidat pour 'profession': 'COIFFEUSE'\n",
                        "[INFO]   'profession' = 'COIFFEUSE' (extrait par ancre)\n",
                        "[INFO] Extraction positionnelle pour les champs manquants:\n",
                        "[DEBUG]     'REPUBLIQUE DU CAMEROUN' identifié comme label (similaire à 'REPUBLIQUE', score=0.78)\n",
                        "[DEBUG]     'REPUBLIC OF CAMEROON' identifié comme label (mots clés: 3/3)\n",
                        "[DEBUG]     'NOM/SURNAME' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'PRENOMS/GIVEN NAMES' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'LIEU DE NAISSANCE/PLACE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'TAILLE/HEIGHT' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     'PROFESSION/OCCUPATION' identifié comme label (similaire à 'PROFESSION', score=0.79)\n",
                        "[DEBUG]     'SIGNATURE' identifié comme label (similaire à 'SIGNATURE', score=1.00)\n",
                        "[INFO] \n",
                        "=== RÉSULTATS FINAUX ===\n",
                        "[INFO]   [SUCCESS] nom: AKONDACK AITAOY\n",
                        "[INFO]   [SUCCESS] prenom: YVETTE MARIAN\n",
                        "[INFO]   [SUCCESS] date_naissance: 19.04.1999\n",
                        "[INFO]   [SUCCESS] lieu_naissance: BAFIA\n",
                        "[INFO]   [SUCCESS] sexe: F\n",
                        "[INFO]   [SUCCESS] taille: 1,73\n",
                        "[INFO]   [SUCCESS] profession: COIFFEUSE\n",
                        "[INFO] Confiance: 117%\n",
                        "\n",
                        "==================================================\n",
                        "RÉSUMÉ DE L'EXTRACTION\n",
                        "==================================================\n",
                        "Extraction réussie (confiance: 117%)\n",
                        "Qualité OCR: 0.92\n",
                        "Ancres détectées: {'nom': True, 'prenom': True, 'lieu_naissance': True, 'profession': True}\n",
                        "\n",
                        "Données extraites:\n",
                        "  nom: AKONDACK AITAOY\n",
                        "  prenom: YVETTE MARIAN\n",
                        "  date_naissance: 19.04.1999\n",
                        "  lieu_naissance: BAFIA\n",
                        "  sexe: F\n",
                        "  taille: 1,73\n",
                        "  profession: COIFFEUSE\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "class CNIExtractor18F:\n",
                "    \"\"\"\n",
                "    Extracteur de CNI camerounaise utilisant une approche par élimination\n",
                "    avec détection floue des ancres.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, quality_threshold: float = 0.5, similarity_threshold: float = 0.70, debug: bool = True):\n",
                "        \"\"\"\n",
                "        Initialise l'extracteur.\n",
                "        \n",
                "        Args:\n",
                "            quality_threshold: Seuil minimal de qualité OCR pour procéder\n",
                "            similarity_threshold: Seuil de similarité pour la détection des ancres\n",
                "            debug: Active les logs de débogage\n",
                "        \"\"\"\n",
                "        self.quality_threshold = quality_threshold\n",
                "        self.similarity_threshold = similarity_threshold\n",
                "        self.debug = debug\n",
                "        \n",
                "        # Ancres possibles pour chaque champ\n",
                "        self.anchors = {\n",
                "            'nom': ['NOM', 'SURNAME', 'NOM/SURNAME'],\n",
                "            'prenom': ['PRENOMS', 'PRENOM', 'GIVEN NAMES', 'GIVEN NAME', 'PRENOMS/GIVEN NAMES'],\n",
                "            'lieu_naissance': ['LIEU DE NAISSANCE', 'PLACE OF BIRTH', \n",
                "                              'LIEU DENAISSANCE', 'PLACEOF BIRTH',\n",
                "                              'LIEU DE NAISSANCE/PLACEOF BIRTH'],\n",
                "            'profession': ['PROFESSION', 'OCCUPATION', 'PROFESSION/OCCUPATION']\n",
                "        }\n",
                "        \n",
                "        # Tous les labels possibles (pour filtrage)\n",
                "        self.all_labels = set()\n",
                "        for labels in self.anchors.values():\n",
                "            self.all_labels.update(labels)\n",
                "        self.all_labels.update(['DATE DE NAISSANCE', 'DATE OF BIRTH', \n",
                "                                'SEXE', 'SEX', 'TAILLE', 'HEIGHT',\n",
                "                                'REPUBLIQUE', 'CAMEROUN', 'REPUBLIC', 'CAMEROON',\n",
                "                                'CARTE', 'NATIONALE', 'IDENTITE', 'IDENTITY', 'CARD',\n",
                "                                'PROFESSION', 'OCCUPATION', 'SIGNATURE'])\n",
                "    \n",
                "    def log(self, message: str, level: str = \"INFO\"):\n",
                "        \"\"\"Affiche un message de log si le mode debug est activé.\"\"\"\n",
                "        if self.debug:\n",
                "            print(f\"[{level}] {message}\")\n",
                "    \n",
                "    def assess_quality(self, ocr_data: Dict) -> Tuple[bool, float]:\n",
                "        \"\"\"\n",
                "        Évalue la qualité globale des données OCR.\n",
                "        \n",
                "        Returns:\n",
                "            (peut_continuer, score_qualite)\n",
                "        \"\"\"\n",
                "        self.log(\"=== ÉVALUATION DE LA QUALITÉ ===\")\n",
                "        \n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "        \n",
                "        if not scores or not texts:\n",
                "            self.log(\"Pas de données OCR\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "        \n",
                "        # Filtrer les scores valides (> 0)\n",
                "        valid_scores = [s for s in scores if s > 0]\n",
                "        \n",
                "        if not valid_scores:\n",
                "            self.log(\"Aucun score valide\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "        \n",
                "        # Calculer le score moyen\n",
                "        avg_score = sum(valid_scores) / len(valid_scores)\n",
                "        \n",
                "        # Compter les éléments de bonne qualité\n",
                "        good_quality = sum(1 for s in scores if s > 0.7)\n",
                "        \n",
                "        self.log(f\"Score moyen: {avg_score:.2f}\")\n",
                "        self.log(f\"Éléments de bonne qualité (>0.7): {good_quality}/{len(scores)}\")\n",
                "        self.log(f\"Éléments valides: {len(valid_scores)}\")\n",
                "        \n",
                "        # Au moins 8 éléments détectés et score moyen acceptable\n",
                "        can_proceed = (len(valid_scores) >= 8 and \n",
                "                      avg_score >= self.quality_threshold and\n",
                "                      good_quality >= 5)\n",
                "        \n",
                "        self.log(f\"Peut continuer: {can_proceed}\")\n",
                "        \n",
                "        return can_proceed, avg_score\n",
                "    \n",
                "    def preprocess(self, texts: List[str], scores: List[float], \n",
                "                  polygons: List) -> List[Tuple[str, float, List]]:\n",
                "        \"\"\"\n",
                "        Prétraite les données OCR en filtrant le bruit.\n",
                "        \n",
                "        Returns:\n",
                "            Liste de tuples (texte, score, polygone) nettoyés\n",
                "        \"\"\"\n",
                "        self.log(\"=== PRÉPROCESSING ===\")\n",
                "        processed = []\n",
                "        \n",
                "        for i, (text, score, polygon) in enumerate(zip(texts, scores, polygons)):\n",
                "            original_text = text\n",
                "            \n",
                "            # Filtrer les scores trop faibles\n",
                "            if score < 0.3:\n",
                "                self.log(f\"  Filtré (score faible): '{text}' (score={score:.2f})\", \"DEBUG\")\n",
                "                continue\n",
                "            \n",
                "            # Filtrer les textes vides\n",
                "            text = text.strip()\n",
                "            if not text:\n",
                "                self.log(f\"  Filtré (texte vide): index {i}\", \"DEBUG\")\n",
                "                continue\n",
                "            \n",
                "            # Filtrer les caractères non-latins isolés\n",
                "            if len(text) <= 2:\n",
                "                if any(ord(c) > 127 for c in text):\n",
                "                    self.log(f\"  Filtré (caractère non-latin): '{text}'\", \"DEBUG\")\n",
                "                    continue\n",
                "            \n",
                "            processed.append((text, score, polygon))\n",
                "            self.log(f\"  Gardé: '{text}' (score={score:.2f})\")\n",
                "        \n",
                "        self.log(f\"Éléments après préprocessing: {len(processed)}/{len(texts)}\")\n",
                "        \n",
                "        return processed\n",
                "    \n",
                "    def similarity_score(self, str1: str, str2: str) -> float:\n",
                "        \"\"\"\n",
                "        Calcule le score de similarité entre deux chaînes (Jaro-Winkler approximé).\n",
                "        \"\"\"\n",
                "        # Normalisation\n",
                "        s1 = str1.upper().strip()\n",
                "        s2 = str2.upper().strip()\n",
                "        \n",
                "        # SequenceMatcher donne un bon compromis\n",
                "        base_score = SequenceMatcher(None, s1, s2).ratio()\n",
                "        \n",
                "        # Bonus si les premiers caractères correspondent (Jaro-Winkler like)\n",
                "        prefix_match = 0\n",
                "        for i in range(min(4, len(s1), len(s2))):\n",
                "            if s1[i] == s2[i]:\n",
                "                prefix_match += 1\n",
                "            else:\n",
                "                break\n",
                "        \n",
                "        # Ajuster le score avec le bonus de préfixe\n",
                "        final_score = base_score + (prefix_match * 0.1 * (1 - base_score))\n",
                "        \n",
                "        return min(final_score, 1.0)\n",
                "    \n",
                "    def extract_fixed_format_fields(self, data: List[Tuple[str, float, List]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait les champs à format fixe (date, sexe, taille).\n",
                "        \n",
                "        Returns:\n",
                "            Dictionnaire avec les champs extraits et les indices à retirer\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS À FORMAT FIXE ===\")\n",
                "        \n",
                "        results = {\n",
                "            'date_naissance': None,\n",
                "            'sexe': None,\n",
                "            'taille': None\n",
                "        }\n",
                "        indices_to_remove = []\n",
                "        \n",
                "        # Patterns de détection\n",
                "        date_pattern = re.compile(r'^\\d{1,2}[./]\\d{1,2}[./]\\d{4}$')\n",
                "        taille_pattern = re.compile(r'^[12][,.]?\\d{2}$')\n",
                "        \n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            # Date de naissance\n",
                "            if not results['date_naissance'] and date_pattern.match(text):\n",
                "                self.log(f\"  Date trouvée: '{text}' à l'index {idx}\")\n",
                "                results['date_naissance'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Sexe\n",
                "            if not results['sexe'] and text in ['M', 'F']:\n",
                "                self.log(f\"  Sexe trouvé: '{text}' à l'index {idx}\")\n",
                "                results['sexe'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Taille\n",
                "            if not results['taille'] and taille_pattern.match(text):\n",
                "                # Normaliser le format\n",
                "                taille = text.replace('.', ',')\n",
                "                if ',' not in taille and len(taille) == 3:\n",
                "                    taille = taille[0] + ',' + taille[1:]\n",
                "                self.log(f\"  Taille trouvée: '{text}' -> '{taille}' à l'index {idx}\")\n",
                "                results['taille'] = taille\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "        \n",
                "        self.log(f\"Champs fixes extraits: Date={results['date_naissance']}, Sexe={results['sexe']}, Taille={results['taille']}\")\n",
                "        self.log(f\"Indices à retirer: {indices_to_remove}\")\n",
                "        \n",
                "        # Retirer les éléments extraits de la liste\n",
                "        results['indices_removed'] = indices_to_remove\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def detect_anchors(self, data: List[Tuple[str, float, List]]) -> Dict[str, List[Tuple[int, str, float]]]:\n",
                "        \"\"\"\n",
                "        Détecte les ancres avec similarité floue.\n",
                "        \n",
                "        Returns:\n",
                "            Dictionnaire {type_champ: [(indice, texte_trouvé, score_similarité)]}\n",
                "        \"\"\"\n",
                "        self.log(\"=== DÉTECTION DES ANCRES ===\")\n",
                "        \n",
                "        detected = {field: [] for field in self.anchors}\n",
                "        \n",
                "        for idx, (text, score, _) in enumerate(data):\n",
                "            text_upper = text.upper()\n",
                "            \n",
                "            # Vérifier contre chaque type d'ancre\n",
                "            for field, anchor_list in self.anchors.items():\n",
                "                for anchor in anchor_list:\n",
                "                    sim_score = self.similarity_score(text_upper, anchor)\n",
                "                    \n",
                "                    if sim_score >= self.similarity_threshold:\n",
                "                        self.log(f\"  Ancre détectée pour '{field}': '{text}' ~ '{anchor}' (similarité={sim_score:.2f})\")\n",
                "                        detected[field].append((idx, text, sim_score))\n",
                "                        break\n",
                "                    elif sim_score > 0.7:  # Log les correspondances proches\n",
                "                        self.log(f\"    Correspondance proche pour '{field}': '{text}' ~ '{anchor}' (similarité={sim_score:.2f})\", \"DEBUG\")\n",
                "        \n",
                "        # Résumé des ancres détectées\n",
                "        for field, anchors in detected.items():\n",
                "            if anchors:\n",
                "                self.log(f\"  {field}: {len(anchors)} ancre(s) trouvée(s)\")\n",
                "            else:\n",
                "                self.log(f\"  {field}: aucune ancre trouvée\", \"WARNING\")\n",
                "        \n",
                "        return detected\n",
                "    \n",
                "    def is_likely_label(self, text: str) -> bool:\n",
                "        \"\"\"\n",
                "        Vérifie si un texte ressemble à un label plutôt qu'à une valeur.\n",
                "        \"\"\"\n",
                "        text_upper = text.upper()\n",
                "        \n",
                "        # Vérifier si le texte contient un slash (caractéristique des labels bilingues)\n",
                "        if '/' in text and any(word in text_upper for word in ['NOM', 'SURNAME', 'PRENOM', 'GIVEN', 'DATE', 'LIEU', 'PLACE', 'SEXE', 'SEX', 'TAILLE', 'HEIGHT']):\n",
                "            self.log(f\"    '{text}' identifié comme label (format bilingue avec /)\", \"DEBUG\")\n",
                "            return True\n",
                "        \n",
                "        # Vérifier la similarité avec tous les labels connus\n",
                "        for label in self.all_labels:\n",
                "            sim = self.similarity_score(text_upper, label)\n",
                "            if sim >= 0.75:\n",
                "                self.log(f\"    '{text}' identifié comme label (similaire à '{label}', score={sim:.2f})\", \"DEBUG\")\n",
                "                return True\n",
                "        \n",
                "        # Vérifier les patterns de labels composés\n",
                "        label_words = ['CARTE', 'NATIONALE', 'REPUBLIQUE', 'DATE', 'LIEU', \n",
                "                      'PLACE', 'BIRTH', 'NAISSANCE', 'IDENTITY', 'CARD',\n",
                "                      'REPUBLIC', 'CAMEROON', 'CAMEROUN', 'OF', 'DE', 'DU',\n",
                "                      'PRENOMS', 'PRENOM', 'GIVEN', 'NAMES', 'NAME', 'NOM', 'SURNAME']\n",
                "        \n",
                "        words = text_upper.split()\n",
                "        if len(words) > 1:\n",
                "            matches = sum(1 for word in words if word in label_words)\n",
                "            if matches >= len(words) / 2:\n",
                "                self.log(f\"    '{text}' identifié comme label (mots clés: {matches}/{len(words)})\", \"DEBUG\")\n",
                "                return True\n",
                "        \n",
                "        # Vérifier si c'est exactement un mot clé\n",
                "        if text_upper in label_words:\n",
                "            self.log(f\"    '{text}' identifié comme label (mot clé exact)\", \"DEBUG\")\n",
                "            return True\n",
                "        \n",
                "        return False\n",
                "    \n",
                "    def extract_by_proximity(self, data: List[Tuple[str, float, List]], \n",
                "                            anchor_idx: int, field_name: str) -> Optional[str]:\n",
                "        \"\"\"\n",
                "        Extrait la valeur la plus proche d'une ancre.\n",
                "        \"\"\"\n",
                "        self.log(f\"  Recherche de valeur pour '{field_name}' près de l'ancre à l'index {anchor_idx}\")\n",
                "        \n",
                "        if anchor_idx >= len(data):\n",
                "            return None\n",
                "        \n",
                "        anchor_text = data[anchor_idx][0]\n",
                "        anchor_polygon = data[anchor_idx][2]\n",
                "        anchor_center = self.calculate_center(anchor_polygon)\n",
                "        \n",
                "        self.log(f\"    Ancre: '{anchor_text}' au centre {anchor_center}\")\n",
                "        \n",
                "        candidates = []\n",
                "        \n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            if idx == anchor_idx:\n",
                "                continue\n",
                "            \n",
                "            # Ignorer les labels\n",
                "            if self.is_likely_label(text):\n",
                "                self.log(f\"    Ignoré (label): '{text}'\", \"DEBUG\")\n",
                "                continue\n",
                "            \n",
                "            value_center = self.calculate_center(polygon)\n",
                "            \n",
                "            # Calculer la distance\n",
                "            distance = ((value_center[0] - anchor_center[0])**2 + \n",
                "                       (value_center[1] - anchor_center[1])**2) ** 0.5\n",
                "            \n",
                "            # Vérifier la position relative (à droite ou en dessous)\n",
                "            is_right = value_center[0] > anchor_center[0]\n",
                "            is_below = value_center[1] > anchor_center[1]\n",
                "            \n",
                "            if is_right or is_below:\n",
                "                # Prioriser les éléments proches\n",
                "                proximity_score = 1 / (1 + distance/100)\n",
                "                candidates.append({\n",
                "                    'text': text,\n",
                "                    'score': score * proximity_score,\n",
                "                    'distance': distance,\n",
                "                    'position': 'droite' if is_right else 'dessous'\n",
                "                })\n",
                "                self.log(f\"    Candidat: '{text}' ({candidates[-1]['position']}, distance={distance:.0f}, score={candidates[-1]['score']:.2f})\")\n",
                "        \n",
                "        if candidates:\n",
                "            # Prendre le meilleur candidat\n",
                "            best = max(candidates, key=lambda x: x['score'])\n",
                "            self.log(f\"  Meilleur candidat pour '{field_name}': '{best['text']}'\")\n",
                "            return best['text']\n",
                "        \n",
                "        self.log(f\"  Aucun candidat trouvé pour '{field_name}'\", \"WARNING\")\n",
                "        return None\n",
                "    \n",
                "    def calculate_center(self, polygon: List[List[int]]) -> Tuple[float, float]:\n",
                "        \"\"\"Calcule le centre d'un polygone.\"\"\"\n",
                "        x_coords = [p[0] for p in polygon]\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        return (sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords))\n",
                "    \n",
                "    def extract_remaining_fields(self, data: List[Tuple[str, float, List]], \n",
                "                                anchors: Dict[str, List[Tuple[int, str, float]]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait nom, prénom et lieu de naissance.\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS RESTANTS ===\")\n",
                "        \n",
                "        results = {\n",
                "            'nom': None,\n",
                "            'prenom': None,\n",
                "            'lieu_naissance': None,\n",
                "            'profession': None,\n",
                "        }\n",
                "        \n",
                "        used_indices = set()\n",
                "        used_values = set()  # Pour éviter les doublons\n",
                "        \n",
                "        # Si on a des ancres, les utiliser\n",
                "        for field in ['nom', 'prenom', 'lieu_naissance', 'profession']:\n",
                "            if anchors[field]:\n",
                "                # Prendre l'ancre avec le meilleur score de similarité\n",
                "                best_anchor = max(anchors[field], key=lambda x: x[2])\n",
                "                anchor_idx = best_anchor[0]\n",
                "                \n",
                "                self.log(f\"Extraction par ancre pour '{field}':\")\n",
                "                value = self.extract_by_proximity(data, anchor_idx, field)\n",
                "                \n",
                "                if value and not self.is_likely_label(value) and value not in used_values:\n",
                "                    results[field] = value\n",
                "                    used_values.add(value)\n",
                "                    # Marquer l'indice comme utilisé\n",
                "                    for idx, (text, _, _) in enumerate(data):\n",
                "                        if text == value:\n",
                "                            used_indices.add(idx)\n",
                "                            self.log(f\"  '{field}' = '{value}' (extrait par ancre)\")\n",
                "                            break\n",
                "                else:\n",
                "                    self.log(f\"  Échec de l'extraction par ancre pour '{field}'\", \"WARNING\")\n",
                "        \n",
                "        # Pour les champs manquants, utiliser l'analyse positionnelle\n",
                "        self.log(\"Extraction positionnelle pour les champs manquants:\")\n",
                "        \n",
                "        remaining_texts = []\n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            if idx not in used_indices and not self.is_likely_label(text) and text not in used_values:\n",
                "                y_pos = self.calculate_center(polygon)[1]\n",
                "                remaining_texts.append({\n",
                "                    'text': text,\n",
                "                    'score': score,\n",
                "                    'y_position': y_pos,\n",
                "                    'index': idx\n",
                "                })\n",
                "                self.log(f\"  Texte restant: '{text}' (y={y_pos:.0f}, score={score:.2f})\")\n",
                "        \n",
                "        # Trier par position verticale\n",
                "        remaining_texts.sort(key=lambda x: x['y_position'])\n",
                "        \n",
                "        # Assigner les champs manquants par position\n",
                "        if not results['nom'] and remaining_texts:\n",
                "            results['nom'] = remaining_texts[0]['text']\n",
                "            used_values.add(remaining_texts[0]['text'])\n",
                "            self.log(f\"  'nom' = '{results['nom']}' (position: premier élément)\")\n",
                "            remaining_texts.pop(0)\n",
                "        \n",
                "        if not results['prenom'] and remaining_texts:\n",
                "            results['prenom'] = remaining_texts[0]['text']\n",
                "            used_values.add(remaining_texts[0]['text'])\n",
                "            self.log(f\"  'prenom' = '{results['prenom']}' (position: deuxième élément)\")\n",
                "            remaining_texts.pop(0)\n",
                "        \n",
                "        if not results['lieu_naissance'] and remaining_texts:\n",
                "            # Le lieu est souvent après le nom/prénom\n",
                "            results['lieu_naissance'] = remaining_texts[0]['text']\n",
                "            self.log(f\"  'lieu_naissance' = '{results['lieu_naissance']}' (position: troisième élément)\")\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, any]:\n",
                "        \"\"\"\n",
                "        Méthode principale d'extraction.\n",
                "        \n",
                "        Returns:\n",
                "            Dictionnaire avec les champs extraits et les métadonnées\n",
                "        \"\"\"\n",
                "        self.log(\"=\"*50)\n",
                "        self.log(\"DÉBUT DE L'EXTRACTION CNI\")\n",
                "        self.log(\"=\"*50)\n",
                "        \n",
                "        # 1. Évaluer la qualité\n",
                "        can_proceed, quality_score = self.assess_quality(ocr_data)\n",
                "        \n",
                "        if not can_proceed:\n",
                "            return {\n",
                "                'success': False,\n",
                "                'quality_score': quality_score,\n",
                "                'message': 'Qualité OCR insuffisante',\n",
                "                'data': {}\n",
                "            }\n",
                "        \n",
                "        # 2. Prétraitement\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        polygons = ocr_data.get('rec_polys', [])\n",
                "        \n",
                "        processed_data = self.preprocess(texts, scores, polygons)\n",
                "        \n",
                "        # 3. Extraction des champs à format fixe\n",
                "        fixed_fields = self.extract_fixed_format_fields(processed_data)\n",
                "        \n",
                "        # Retirer les éléments extraits\n",
                "        remaining_data = [\n",
                "            item for idx, item in enumerate(processed_data) \n",
                "            if idx not in fixed_fields['indices_removed']\n",
                "        ]\n",
                "        \n",
                "        self.log(f\"\\nÉléments restants après extraction des champs fixes: {len(remaining_data)}\")\n",
                "        for text, score, _ in remaining_data:\n",
                "            self.log(f\"  - '{text}' (score={score:.2f})\")\n",
                "        \n",
                "        # 4. Détecter les ancres\n",
                "        anchors = self.detect_anchors(remaining_data)\n",
                "        \n",
                "        # 5. Extraire les champs restants\n",
                "        remaining_fields = self.extract_remaining_fields(remaining_data, anchors)\n",
                "        \n",
                "        # 6. Consolider les résultats\n",
                "        extracted_data = {\n",
                "            'nom': remaining_fields['nom'],\n",
                "            'prenom': remaining_fields['prenom'],\n",
                "            'date_naissance': fixed_fields['date_naissance'],\n",
                "            'lieu_naissance': remaining_fields['lieu_naissance'],\n",
                "            'sexe': fixed_fields['sexe'],\n",
                "            'taille': fixed_fields['taille'],\n",
                "            'profession': remaining_fields['profession']\n",
                "        }\n",
                "        \n",
                "        # Calculer le score de confiance\n",
                "        filled_fields = sum(1 for v in extracted_data.values() if v is not None)\n",
                "        confidence = filled_fields / 6.0\n",
                "        \n",
                "        self.log(\"\\n=== RÉSULTATS FINAUX ===\")\n",
                "        for field, value in extracted_data.items():\n",
                "            status = \"[SUCCESS]\" if value else \"[FAILURE]\"\n",
                "            self.log(f\"  {status} {field}: {value}\")\n",
                "        self.log(f\"Confiance: {confidence:.0%}\")\n",
                "        \n",
                "        return {\n",
                "            'success': True,\n",
                "            'quality_score': quality_score,\n",
                "            'confidence': confidence,\n",
                "            'anchors_detected': {k: len(v) > 0 for k, v in anchors.items()},\n",
                "            'data': extracted_data\n",
                "        }\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    import json\n",
                "    \n",
                "    # Charger les données OCR\n",
                "    with open(\"output/images/old/resizedold_front_02_res.json\", 'r') as f:\n",
                "        ocr_data = json.load(f)\n",
                "    \n",
                "    # Créer l'extracteur avec debug activé\n",
                "    extractor = CNIExtractor18F(debug=True)\n",
                "    \n",
                "    # Extraire les informations\n",
                "    result = extractor.extract(ocr_data)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"RÉSUMÉ DE L'EXTRACTION\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    if result['success']:\n",
                "        print(f\"Extraction réussie (confiance: {result['confidence']:.0%})\")\n",
                "        print(f\"Qualité OCR: {result['quality_score']:.2f}\")\n",
                "        print(f\"Ancres détectées: {result['anchors_detected']}\")\n",
                "        print(\"\\nDonnées extraites:\")\n",
                "        for field, value in result['data'].items():\n",
                "            print(f\"  {field}: {value}\")\n",
                "    else:\n",
                "        print(f\"Extraction échouée: {result['message']}\")\n",
                "        print(f\"Qualité OCR: {result['quality_score']:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "73c3f009",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] ==================================================\n",
                        "[INFO] DÉBUT DE L'EXTRACTION CNI (VERSO)\n",
                        "[INFO] ==================================================\n",
                        "[INFO] === ÉVALUATION DE LA QUALITÉ ===\n",
                        "[INFO] Score moyen: 0.93\n",
                        "[INFO] Éléments de bonne qualité (>0.7): 26/29\n",
                        "[INFO] Éléments valides: 28\n",
                        "[INFO] Peut continuer: True\n",
                        "[INFO] === PRÉPROCESSING (MINIMAL) ===\n",
                        "[DEBUG]   Filtré (caractère non-latin): '鸡'\n",
                        "[DEBUG]   Filtré (score faible): '' (score=0.00)\n",
                        "[INFO]   Gardé: 'PERE/FATHER' (score=0.96)\n",
                        "[INFO]   Gardé: 'TSOYIIA DEEDI RENE' (score=0.91)\n",
                        "[INFO]   Gardé: '0' (score=0.98)\n",
                        "[INFO]   Gardé: 'MERE/MOTHER' (score=0.98)\n",
                        "[INFO]   Gardé: 'KAMANA ROSALIE' (score=0.98)\n",
                        "[INFO]   Gardé: 'S.P./S.M.' (score=0.95)\n",
                        "[INFO]   Gardé: 'AUTORITE/AUTHORITY' (score=0.99)\n",
                        "[INFO]   Gardé: 'DATE DE DELIVRANCEI' (score=0.93)\n",
                        "[INFO]   Gardé: 'POSTE DIDENTIFICATIONA' (score=0.94)\n",
                        "[INFO]   Gardé: 'DATE OFISSUE' (score=0.96)\n",
                        "[INFO]   Gardé: '860000' (score=1.00)\n",
                        "[INFO]   Gardé: 'IDENTIFICATION POST' (score=0.95)\n",
                        "[INFO]   Gardé: '25.08.2017' (score=0.98)\n",
                        "[INFO]   Gardé: 'LT02' (score=0.98)\n",
                        "[INFO]   Gardé: 'ADRESSE/ADDRESS' (score=0.96)\n",
                        "[INFO]   Gardé: 'DATED'EXPIRATION/' (score=0.97)\n",
                        "[INFO]   Gardé: 'IDENTIFIANTUNIQUEI' (score=0.97)\n",
                        "[INFO]   Gardé: 'DLA-NDOGPASSI' (score=0.99)\n",
                        "[INFO]   Gardé: 'DATEOF EXPIRY' (score=0.96)\n",
                        "[INFO]   Gardé: 'UNIQUEIDENTIFIER' (score=0.98)\n",
                        "[DEBUG]   Filtré (caractère non-latin): '川'\n",
                        "[INFO]   Gardé: '25.08.2027' (score=0.97)\n",
                        "[INFO]   Gardé: '20170575878920923' (score=1.00)\n",
                        "[INFO]   Gardé: 'Martin MBARGA NGUELE' (score=0.94)\n",
                        "[INFO]   Gardé: 'CAMEROUN' (score=0.82)\n",
                        "[INFO]   Gardé: 'CAMEROON' (score=0.96)\n",
                        "[INFO]   Gardé: '000080302' (score=0.99)\n",
                        "[INFO] Éléments après préprocessing: 26/29\n",
                        "[INFO] === EXTRACTION DES CHAMPS À FORMAT FIXE ===\n",
                        "[INFO]   Date trouvée: '25.08.2017' à l'index 12\n",
                        "[INFO]   Code poste trouvé: 'LT02' à l'index 13\n",
                        "[INFO]   Date trouvée: '25.08.2027' à l'index 20\n",
                        "[INFO]   Identifiant unique trouvé: '20170575878920923' à l'index 21\n",
                        "[INFO]   Numéro CNI trouvé: '000080302' à l'index 25\n",
                        "[INFO] Champs fixes extraits: 5 sur 5\n",
                        "[INFO] Indices à retirer: [12, 13, 20, 21, 25]\n",
                        "[INFO] \n",
                        "Éléments restants après extraction des champs fixes: 21\n",
                        "[INFO] === DÉTECTION DES ANCRES ===\n",
                        "[INFO]   Ancre détectée pour 'pere': 'PERE/FATHER' ~ 'PERE' (similarité=0.72)\n",
                        "[INFO]   Ancre détectée pour 'mere': 'PERE/FATHER' ~ 'MERE/MOTHER' (similarité=0.73)\n",
                        "[INFO]   Ancre détectée pour 'pere': 'MERE/MOTHER' ~ 'PERE/FATHER' (similarité=0.73)\n",
                        "[INFO]   Ancre détectée pour 'mere': 'MERE/MOTHER' ~ 'MERE' (similarité=0.72)\n",
                        "[INFO]   Ancre détectée pour 'autorite': 'AUTORITE/AUTHORITY' ~ 'AUTORITE' (similarité=0.77)\n",
                        "[INFO]   Ancre détectée pour 'date_delivrance': 'DATE DE DELIVRANCEI' ~ 'DATE DE DELIVRANCE' (similarité=0.98)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATE DE DELIVRANCEI' ~ 'DATE D'EXPIRATION' (similarité=0.77)\n",
                        "[INFO]   Ancre détectée pour 'poste_identification': 'POSTE DIDENTIFICATIONA' ~ 'POSTE D'IDENTIFICATION' (similarité=0.97)\n",
                        "[INFO]   Ancre détectée pour 'date_delivrance': 'DATE OFISSUE' ~ 'DATE OF ISSUE' (similarité=0.98)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATE OFISSUE' ~ 'DATE OF EXPIRY' (similarité=0.77)\n",
                        "[INFO]   Ancre détectée pour 'poste_identification': 'IDENTIFICATION POST' ~ 'IDENTIFICATION POST' (similarité=1.00)\n",
                        "[INFO]   Ancre détectée pour 'identifiant_unique': 'IDENTIFICATION POST' ~ 'IDENTIFIANT UNIQUE' (similarité=0.76)\n",
                        "[INFO]   Ancre détectée pour 'adresse': 'ADRESSE/ADDRESS' ~ 'ADRESSE' (similarité=0.78)\n",
                        "[INFO]   Ancre détectée pour 'date_delivrance': 'DATED'EXPIRATION/' ~ 'DATE DE DELIVRANCE' (similarité=0.74)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATED'EXPIRATION/' ~ 'DATE D'EXPIRATION' (similarité=0.96)\n",
                        "[INFO]   Ancre détectée pour 'poste_identification': 'IDENTIFIANTUNIQUEI' ~ 'IDENTIFICATION POST' (similarité=0.76)\n",
                        "[INFO]   Ancre détectée pour 'identifiant_unique': 'IDENTIFIANTUNIQUEI' ~ 'IDENTIFIANT UNIQUE' (similarité=0.97)\n",
                        "[INFO]   Ancre détectée pour 'date_delivrance': 'DATEOF EXPIRY' ~ 'DATE DE DELIVRANCE' (similarité=0.71)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATEOF EXPIRY' ~ 'DATE D'EXPIRATION' (similarité=0.80)\n",
                        "[INFO]   Ancre détectée pour 'identifiant_unique': 'UNIQUEIDENTIFIER' ~ 'UNIQUE IDENTIFIER' (similarité=0.98)\n",
                        "[INFO]   pere: 2 ancre(s) trouvée(s)\n",
                        "[INFO]   mere: 2 ancre(s) trouvée(s)\n",
                        "[INFO]   date_delivrance: 4 ancre(s) trouvée(s)\n",
                        "[INFO]   date_expiration: 4 ancre(s) trouvée(s)\n",
                        "[INFO]   adresse: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   poste_identification: 3 ancre(s) trouvée(s)\n",
                        "[INFO]   identifiant_unique: 3 ancre(s) trouvée(s)\n",
                        "[INFO]   autorite: 1 ancre(s) trouvée(s)\n",
                        "[INFO] === EXTRACTION DES CHAMPS RESTANTS ===\n",
                        "[INFO] Extraction par ancre pour 'pere':\n",
                        "[INFO]   Recherche de valeur pour 'pere' près de l'ancre à l'index 3\n",
                        "[DEBUG]     'PERE/FATHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PERE/FATHER'\n",
                        "[INFO]     Candidat: 'TSOYIIA DEEDI RENE' (distance=213, score=0.29)\n",
                        "[INFO]     Candidat: '0' (distance=1681, score=0.05)\n",
                        "[INFO]     Candidat: 'KAMANA ROSALIE' (distance=111, score=0.46)\n",
                        "[DEBUG]     'S.P./S.M.' identifié comme label (similaire à 'S.P.', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'S.P./S.M.'\n",
                        "[DEBUG]     'AUTORITE/AUTHORITY' identifié comme label (similaire à 'AUTHORITY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'AUTORITE/AUTHORITY'\n",
                        "[DEBUG]     'DATE DE DELIVRANCEI' identifié comme label (similaire à 'DATE DE DELIVRANCEI', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE DELIVRANCEI'\n",
                        "[DEBUG]     'POSTE DIDENTIFICATIONA' identifié comme label (similaire à 'POSTE D'IDENTIFICATION', score=0.97)\n",
                        "[DEBUG]     Ignoré (label): 'POSTE DIDENTIFICATIONA'\n",
                        "[DEBUG]     'DATE OFISSUE' identifié comme label (similaire à 'DATE OF EXPIRY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'DATE OFISSUE'\n",
                        "[INFO]     Candidat: '860000' (distance=398, score=0.20)\n",
                        "[DEBUG]     'IDENTIFICATION POST' identifié comme label (similaire à 'IDENTIFICATION POST', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFICATION POST'\n",
                        "[DEBUG]     'ADRESSE/ADDRESS' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'ADRESSE/ADDRESS'\n",
                        "[DEBUG]     'DATED'EXPIRATION/' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATED'EXPIRATION/'\n",
                        "[DEBUG]     'IDENTIFIANTUNIQUEI' identifié comme label (similaire à 'IDENTIFICATION POST', score=0.76)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFIANTUNIQUEI'\n",
                        "[INFO]     Candidat: 'DLA-NDOGPASSI' (distance=570, score=0.15)\n",
                        "[DEBUG]     'DATEOF EXPIRY' identifié comme label (similaire à 'DATEOF EXPIRY', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATEOF EXPIRY'\n",
                        "[DEBUG]     'UNIQUEIDENTIFIER' identifié comme label (similaire à 'UNIQUEIDENTIFIERI', score=0.98)\n",
                        "[DEBUG]     Ignoré (label): 'UNIQUEIDENTIFIER'\n",
                        "[INFO]     Candidat: 'Martin MBARGA NGUELE' (distance=1251, score=0.07)\n",
                        "[DEBUG]     'CAMEROUN' identifié comme label (similaire à 'CAMEROON', score=0.93)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROUN'\n",
                        "[DEBUG]     'CAMEROON' identifié comme label (similaire à 'CAMEROON', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROON'\n",
                        "[INFO]   Meilleur candidat pour 'pere': 'KAMANA ROSALIE'\n",
                        "[INFO]   'pere' = 'KAMANA ROSALIE' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'mere':\n",
                        "[INFO]   Recherche de valeur pour 'mere' près de l'ancre à l'index 0\n",
                        "[INFO]     Candidat: 'TSOYIIA DEEDI RENE' (distance=167, score=0.34)\n",
                        "[INFO]     Candidat: '0' (distance=1689, score=0.05)\n",
                        "[DEBUG]     'MERE/MOTHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'MERE/MOTHER'\n",
                        "[INFO]     Candidat: 'KAMANA ROSALIE' (distance=362, score=0.21)\n",
                        "[DEBUG]     'S.P./S.M.' identifié comme label (similaire à 'S.P.', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'S.P./S.M.'\n",
                        "[DEBUG]     'AUTORITE/AUTHORITY' identifié comme label (similaire à 'AUTHORITY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'AUTORITE/AUTHORITY'\n",
                        "[DEBUG]     'DATE DE DELIVRANCEI' identifié comme label (similaire à 'DATE DE DELIVRANCEI', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE DELIVRANCEI'\n",
                        "[DEBUG]     'POSTE DIDENTIFICATIONA' identifié comme label (similaire à 'POSTE D'IDENTIFICATION', score=0.97)\n",
                        "[DEBUG]     Ignoré (label): 'POSTE DIDENTIFICATIONA'\n",
                        "[DEBUG]     'DATE OFISSUE' identifié comme label (similaire à 'DATE OF EXPIRY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'DATE OFISSUE'\n",
                        "[INFO]     Candidat: '860000' (distance=649, score=0.13)\n",
                        "[DEBUG]     'IDENTIFICATION POST' identifié comme label (similaire à 'IDENTIFICATION POST', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFICATION POST'\n",
                        "[DEBUG]     'ADRESSE/ADDRESS' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'ADRESSE/ADDRESS'\n",
                        "[DEBUG]     'DATED'EXPIRATION/' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATED'EXPIRATION/'\n",
                        "[DEBUG]     'IDENTIFIANTUNIQUEI' identifié comme label (similaire à 'IDENTIFICATION POST', score=0.76)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFIANTUNIQUEI'\n",
                        "[INFO]     Candidat: 'DLA-NDOGPASSI' (distance=835, score=0.11)\n",
                        "[DEBUG]     'DATEOF EXPIRY' identifié comme label (similaire à 'DATEOF EXPIRY', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATEOF EXPIRY'\n",
                        "[DEBUG]     'UNIQUEIDENTIFIER' identifié comme label (similaire à 'UNIQUEIDENTIFIERI', score=0.98)\n",
                        "[DEBUG]     Ignoré (label): 'UNIQUEIDENTIFIER'\n",
                        "[INFO]     Candidat: 'Martin MBARGA NGUELE' (distance=1440, score=0.06)\n",
                        "[DEBUG]     'CAMEROUN' identifié comme label (similaire à 'CAMEROON', score=0.93)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROUN'\n",
                        "[DEBUG]     'CAMEROON' identifié comme label (similaire à 'CAMEROON', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROON'\n",
                        "[INFO]   Meilleur candidat pour 'mere': 'TSOYIIA DEEDI RENE'\n",
                        "[INFO]   'mere' = 'TSOYIIA DEEDI RENE' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'adresse':\n",
                        "[INFO]   Recherche de valeur pour 'adresse' près de l'ancre à l'index 12\n",
                        "[DEBUG]     'PERE/FATHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PERE/FATHER'\n",
                        "[INFO]     Candidat: 'TSOYIIA DEEDI RENE' (distance=663, score=0.12)\n",
                        "[INFO]     Candidat: '0' (distance=1793, score=0.05)\n",
                        "[DEBUG]     'MERE/MOTHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'MERE/MOTHER'\n",
                        "[INFO]     Candidat: 'KAMANA ROSALIE' (distance=396, score=0.20)\n",
                        "[DEBUG]     'S.P./S.M.' identifié comme label (similaire à 'S.P.', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'S.P./S.M.'\n",
                        "[DEBUG]     'AUTORITE/AUTHORITY' identifié comme label (similaire à 'AUTHORITY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'AUTORITE/AUTHORITY'\n",
                        "[DEBUG]     'DATE DE DELIVRANCEI' identifié comme label (similaire à 'DATE DE DELIVRANCEI', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE DELIVRANCEI'\n",
                        "[DEBUG]     'POSTE DIDENTIFICATIONA' identifié comme label (similaire à 'POSTE D'IDENTIFICATION', score=0.97)\n",
                        "[DEBUG]     Ignoré (label): 'POSTE DIDENTIFICATIONA'\n",
                        "[DEBUG]     'DATE OFISSUE' identifié comme label (similaire à 'DATE OF EXPIRY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'DATE OFISSUE'\n",
                        "[DEBUG]     'IDENTIFICATION POST' identifié comme label (similaire à 'IDENTIFICATION POST', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFICATION POST'\n",
                        "[DEBUG]     'DATED'EXPIRATION/' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATED'EXPIRATION/'\n",
                        "[DEBUG]     'IDENTIFIANTUNIQUEI' identifié comme label (similaire à 'IDENTIFICATION POST', score=0.76)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFIANTUNIQUEI'\n",
                        "[INFO]     Candidat: 'DLA-NDOGPASSI' (distance=92, score=0.52)\n",
                        "[DEBUG]     'DATEOF EXPIRY' identifié comme label (similaire à 'DATEOF EXPIRY', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATEOF EXPIRY'\n",
                        "[DEBUG]     'UNIQUEIDENTIFIER' identifié comme label (similaire à 'UNIQUEIDENTIFIERI', score=0.98)\n",
                        "[DEBUG]     Ignoré (label): 'UNIQUEIDENTIFIER'\n",
                        "[INFO]     Candidat: 'Martin MBARGA NGUELE' (distance=1025, score=0.08)\n",
                        "[DEBUG]     'CAMEROUN' identifié comme label (similaire à 'CAMEROON', score=0.93)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROUN'\n",
                        "[DEBUG]     'CAMEROON' identifié comme label (similaire à 'CAMEROON', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROON'\n",
                        "[INFO]   Meilleur candidat pour 'adresse': 'DLA-NDOGPASSI'\n",
                        "[INFO]   'adresse' = 'DLA-NDOGPASSI' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'autorite':\n",
                        "[INFO]   Recherche de valeur pour 'autorite' près de l'ancre à l'index 6\n",
                        "[DEBUG]     'PERE/FATHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PERE/FATHER'\n",
                        "[INFO]     Candidat: '0' (distance=884, score=0.10)\n",
                        "[DEBUG]     'MERE/MOTHER' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'MERE/MOTHER'\n",
                        "[DEBUG]     'S.P./S.M.' identifié comme label (similaire à 'S.P.', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'S.P./S.M.'\n",
                        "[DEBUG]     'DATE DE DELIVRANCEI' identifié comme label (similaire à 'DATE DE DELIVRANCEI', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE DELIVRANCEI'\n",
                        "[DEBUG]     'POSTE DIDENTIFICATIONA' identifié comme label (similaire à 'POSTE D'IDENTIFICATION', score=0.97)\n",
                        "[DEBUG]     Ignoré (label): 'POSTE DIDENTIFICATIONA'\n",
                        "[DEBUG]     'DATE OFISSUE' identifié comme label (similaire à 'DATE OF EXPIRY', score=0.77)\n",
                        "[DEBUG]     Ignoré (label): 'DATE OFISSUE'\n",
                        "[INFO]     Candidat: '860000' (distance=1059, score=0.09)\n",
                        "[DEBUG]     'IDENTIFICATION POST' identifié comme label (similaire à 'IDENTIFICATION POST', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFICATION POST'\n",
                        "[DEBUG]     'ADRESSE/ADDRESS' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'ADRESSE/ADDRESS'\n",
                        "[DEBUG]     'DATED'EXPIRATION/' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATED'EXPIRATION/'\n",
                        "[DEBUG]     'IDENTIFIANTUNIQUEI' identifié comme label (similaire à 'IDENTIFICATION POST', score=0.76)\n",
                        "[DEBUG]     Ignoré (label): 'IDENTIFIANTUNIQUEI'\n",
                        "[INFO]     Candidat: 'DLA-NDOGPASSI' (distance=960, score=0.09)\n",
                        "[DEBUG]     'DATEOF EXPIRY' identifié comme label (similaire à 'DATEOF EXPIRY', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'DATEOF EXPIRY'\n",
                        "[DEBUG]     'UNIQUEIDENTIFIER' identifié comme label (similaire à 'UNIQUEIDENTIFIERI', score=0.98)\n",
                        "[DEBUG]     Ignoré (label): 'UNIQUEIDENTIFIER'\n",
                        "[INFO]     Candidat: 'Martin MBARGA NGUELE' (distance=504, score=0.16)\n",
                        "[DEBUG]     'CAMEROUN' identifié comme label (similaire à 'CAMEROON', score=0.93)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROUN'\n",
                        "[DEBUG]     'CAMEROON' identifié comme label (similaire à 'CAMEROON', score=1.00)\n",
                        "[DEBUG]     Ignoré (label): 'CAMEROON'\n",
                        "[INFO]   Meilleur candidat pour 'autorite': 'Martin MBARGA NGUELE'\n",
                        "[INFO]   'autorite' = 'Martin MBARGA NGUELE' (extrait par ancre)\n",
                        "[INFO] \n",
                        "=== RÉSULTATS FINAUX ===\n",
                        "[INFO]   [SUCCESS] pere: KAMANA ROSALIE\n",
                        "[INFO]   [SUCCESS] mere: TSOYIIA DEEDI RENE\n",
                        "[INFO]   [SUCCESS] date_delivrance: 25.08.2017\n",
                        "[INFO]   [SUCCESS] date_expiration: 25.08.2027\n",
                        "[INFO]   [SUCCESS] adresse: DLA-NDOGPASSI\n",
                        "[INFO]   [SUCCESS] poste_identification: LT02\n",
                        "[INFO]   [SUCCESS] identifiant_unique: 20170575878920923\n",
                        "[INFO]   [SUCCESS] autorite: Martin MBARGA NGUELE\n",
                        "[INFO]   [SUCCESS] numero_cni: 000080302\n",
                        "[INFO] Confiance: 100%\n",
                        "\n",
                        "==================================================\n",
                        "RÉSUMÉ DE L'EXTRACTION (VERSO)\n",
                        "==================================================\n",
                        "Extraction réussie (confiance: 100%)\n",
                        "Qualité OCR: 0.93\n",
                        "\n",
                        "Données extraites:\n",
                        "  pere: KAMANA ROSALIE\n",
                        "  mere: TSOYIIA DEEDI RENE\n",
                        "  date_delivrance: 25.08.2017\n",
                        "  date_expiration: 25.08.2027\n",
                        "  adresse: DLA-NDOGPASSI\n",
                        "  poste_identification: LT02\n",
                        "  identifiant_unique: 20170575878920923\n",
                        "  autorite: Martin MBARGA NGUELE\n",
                        "  numero_cni: 000080302\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "\n",
                "class CNIExtractor18B:\n",
                "    \"\"\"\n",
                "    Extracteur pour le verso de CNI camerounaise utilisant une approche par élimination\n",
                "    avec détection floue des ancres.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, quality_threshold: float = 0.5, similarity_threshold: float = 0.70, debug: bool = True):\n",
                "        \"\"\"\n",
                "        Initialise l'extracteur pour le verso.\n",
                "\n",
                "        Args:\n",
                "            quality_threshold: Seuil minimal de qualité OCR pour procéder\n",
                "            similarity_threshold: Seuil de similarité pour la détection des ancres\n",
                "            debug: Active les logs de débogage\n",
                "        \"\"\"\n",
                "        self.quality_threshold = quality_threshold\n",
                "        self.similarity_threshold = similarity_threshold\n",
                "        self.debug = debug\n",
                "\n",
                "        # Ancres possibles pour chaque champ du verso\n",
                "        self.anchors = {\n",
                "            'pere': ['PERE', 'FATHER', 'PERE/FATHER'],\n",
                "            'mere': ['MERE', 'MOTHER', 'MERE/MOTHER'],\n",
                "            'date_delivrance': ['DATE DE DELIVRANCE', 'DATE OF ISSUE', \n",
                "                               'DATE DE DELIVRANCEI', 'DATEOFISSUE'],\n",
                "            'date_expiration': ['DATE D\\'EXPIRATION', 'DATE OF EXPIRY',\n",
                "                              'DATED\\'EXPIRATION', 'DATEOF EXPIRY'],\n",
                "            'adresse': ['ADRESSE', 'ADDRESS', 'ADRESSE/ADDRESS'],\n",
                "            'poste_identification': ['POSTE D\\'IDENTIFICATION', 'IDENTIFICATION POST',\n",
                "                                    'POSTE DIDENTIFICATION', 'POSTE DIDENTIFICATIONA'],\n",
                "            'identifiant_unique': ['IDENTIFIANT UNIQUE', 'UNIQUE IDENTIFIER',\n",
                "                                  'IDENTIFIANTUNIQUE', 'UNIQUEIDENTIFIER',\n",
                "                                  'IDENTIFIANTUNIQUEI', 'UNIQUEIDENTIFIERI'],\n",
                "            'autorite': ['AUTORITE', 'AUTHORITY', 'AUTORITE/AUTHORITY']\n",
                "        }\n",
                "\n",
                "        # Tous les labels possibles (pour filtrage)\n",
                "        self.all_labels = set()\n",
                "        for labels in self.anchors.values():\n",
                "            self.all_labels.update(labels)\n",
                "        self.all_labels.update(['S.P.', 'S.M.', 'S.P./S.M.', \n",
                "                                'CAMEROUN', 'CAMEROON'])\n",
                "\n",
                "    def log(self, message: str, level: str = \"INFO\"):\n",
                "        \"\"\"Affiche un message de log si le mode debug est activé.\"\"\"\n",
                "        if self.debug:\n",
                "            print(f\"[{level}] {message}\")\n",
                "\n",
                "    def assess_quality(self, ocr_data: Dict) -> Tuple[bool, float]:\n",
                "        \"\"\"\n",
                "        Évalue la qualité globale des données OCR.\n",
                "\n",
                "        Returns:\n",
                "            (peut_continuer, score_qualite)\n",
                "        \"\"\"\n",
                "        self.log(\"=== ÉVALUATION DE LA QUALITÉ ===\")\n",
                "\n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "\n",
                "        if not scores or not texts:\n",
                "            self.log(\"Pas de données OCR\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "\n",
                "        # Filtrer les scores valides (> 0)\n",
                "        valid_scores = [s for s in scores if s > 0]\n",
                "\n",
                "        if not valid_scores:\n",
                "            self.log(\"Aucun score valide\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "\n",
                "        # Calculer le score moyen\n",
                "        avg_score = sum(valid_scores) / len(valid_scores)\n",
                "\n",
                "        # Compter les éléments de bonne qualité\n",
                "        good_quality = sum(1 for s in scores if s > 0.7)\n",
                "\n",
                "        self.log(f\"Score moyen: {avg_score:.2f}\")\n",
                "        self.log(f\"Éléments de bonne qualité (>0.7): {good_quality}/{len(scores)}\")\n",
                "        self.log(f\"Éléments valides: {len(valid_scores)}\")\n",
                "\n",
                "        # Au moins 5 éléments détectés pour le verso\n",
                "        can_proceed = (len(valid_scores) >= 5 and \n",
                "                      avg_score >= self.quality_threshold and\n",
                "                      good_quality >= 3)\n",
                "\n",
                "        self.log(f\"Peut continuer: {can_proceed}\")\n",
                "\n",
                "        return can_proceed, avg_score\n",
                "\n",
                "    def preprocess(self, texts: List[str], scores: List[float],\n",
                "                  polygons: List) -> List[Tuple[str, float, List]]:\n",
                "        \"\"\"\n",
                "        Prétraite les données OCR en filtrant le bruit.\n",
                "        Pour le moment, traitement minimal comme demandé.\n",
                "        \"\"\"\n",
                "        self.log(\"=== PRÉPROCESSING (MINIMAL) ===\")\n",
                "        processed = []\n",
                "\n",
                "        for i, (text, score, polygon) in enumerate(zip(texts, scores, polygons)):\n",
                "            # Filtrer les scores très faibles\n",
                "            if score < 0.3:\n",
                "                self.log(f\"  Filtré (score faible): '{text}' (score={score:.2f})\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            # Filtrer les textes vides\n",
                "            text = text.strip()\n",
                "            if not text:\n",
                "                self.log(f\"  Filtré (texte vide): index {i}\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            # Filtrer les caractères non-latins isolés (鸡, 川)\n",
                "            if len(text) <= 2:\n",
                "                if any(ord(c) > 127 for c in text):\n",
                "                    self.log(f\"  Filtré (caractère non-latin): '{text}'\", \"DEBUG\")\n",
                "                    continue\n",
                "\n",
                "            processed.append((text, score, polygon))\n",
                "            self.log(f\"  Gardé: '{text}' (score={score:.2f})\")\n",
                "\n",
                "        self.log(f\"Éléments après préprocessing: {len(processed)}/{len(texts)}\")\n",
                "\n",
                "        return processed\n",
                "\n",
                "    def similarity_score(self, str1: str, str2: str) -> float:\n",
                "        \"\"\"\n",
                "        Calcule le score de similarité entre deux chaînes.\n",
                "        \"\"\"\n",
                "        s1 = str1.upper().strip()\n",
                "        s2 = str2.upper().strip()\n",
                "\n",
                "        base_score = SequenceMatcher(None, s1, s2).ratio()\n",
                "\n",
                "        # Bonus pour préfixe identique\n",
                "        prefix_match = 0\n",
                "        for i in range(min(4, len(s1), len(s2))):\n",
                "            if s1[i] == s2[i]:\n",
                "                prefix_match += 1\n",
                "            else:\n",
                "                break\n",
                "\n",
                "        final_score = base_score + (prefix_match * 0.1 * (1 - base_score))\n",
                "\n",
                "        return min(final_score, 1.0)\n",
                "\n",
                "    def extract_fixed_format_fields(self, data: List[Tuple[str, float, List]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait les champs à format fixe (dates, numéros).\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS À FORMAT FIXE ===\")\n",
                "\n",
                "        results = {\n",
                "            'date_delivrance': None,\n",
                "            'date_expiration': None,\n",
                "            'identifiant_unique': None,\n",
                "            'numero_cni': None,\n",
                "            'poste_code': None\n",
                "        }\n",
                "        indices_to_remove = []\n",
                "\n",
                "        # Patterns de détection\n",
                "        date_pattern = re.compile(r'^\\d{1,2}[./]\\d{1,2}[./]\\d{4}$')\n",
                "        id_unique_pattern = re.compile(r'^\\d{15,20}$')  # Identifiant unique long\n",
                "        numero_pattern = re.compile(r'^\\d{9}$')  # Numéro CNI (9 chiffres)\n",
                "        poste_pattern = re.compile(r'^[A-Z]{2}\\d{2}$')  # Code poste (ex: LT02)\n",
                "\n",
                "        dates_found = []\n",
                "        \n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            # Dates (on en attend 2 : délivrance et expiration)\n",
                "            if date_pattern.match(text):\n",
                "                dates_found.append((text, idx))\n",
                "                self.log(f\"  Date trouvée: '{text}' à l'index {idx}\")\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Identifiant unique\n",
                "            if not results['identifiant_unique'] and id_unique_pattern.match(text):\n",
                "                self.log(f\"  Identifiant unique trouvé: '{text}' à l'index {idx}\")\n",
                "                results['identifiant_unique'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Numéro CNI\n",
                "            if not results['numero_cni'] and numero_pattern.match(text):\n",
                "                self.log(f\"  Numéro CNI trouvé: '{text}' à l'index {idx}\")\n",
                "                results['numero_cni'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Code poste\n",
                "            if not results['poste_code'] and poste_pattern.match(text):\n",
                "                self.log(f\"  Code poste trouvé: '{text}' à l'index {idx}\")\n",
                "                results['poste_code'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "\n",
                "        # Assigner les dates (première = délivrance, deuxième = expiration)\n",
                "        if len(dates_found) >= 1:\n",
                "            results['date_delivrance'] = dates_found[0][0]\n",
                "        if len(dates_found) >= 2:\n",
                "            results['date_expiration'] = dates_found[1][0]\n",
                "\n",
                "        self.log(f\"Champs fixes extraits: {len([v for v in results.values() if v])} sur 5\")\n",
                "        self.log(f\"Indices à retirer: {indices_to_remove}\")\n",
                "\n",
                "        results['indices_removed'] = indices_to_remove\n",
                "\n",
                "        return results\n",
                "\n",
                "    def detect_anchors(self, data: List[Tuple[str, float, List]]) -> Dict[str, List[Tuple[int, str, float]]]:\n",
                "        \"\"\"\n",
                "        Détecte les ancres avec similarité floue.\n",
                "        \"\"\"\n",
                "        self.log(\"=== DÉTECTION DES ANCRES ===\")\n",
                "\n",
                "        detected = {field: [] for field in self.anchors}\n",
                "\n",
                "        for idx, (text, score, _) in enumerate(data):\n",
                "            text_upper = text.upper()\n",
                "\n",
                "            for field, anchor_list in self.anchors.items():\n",
                "                for anchor in anchor_list:\n",
                "                    sim_score = self.similarity_score(text_upper, anchor)\n",
                "\n",
                "                    if sim_score >= self.similarity_threshold:\n",
                "                        self.log(f\"  Ancre détectée pour '{field}': '{text}' ~ '{anchor}' (similarité={sim_score:.2f})\")\n",
                "                        detected[field].append((idx, text, sim_score))\n",
                "                        break\n",
                "\n",
                "        # Résumé des ancres détectées\n",
                "        for field, anchors in detected.items():\n",
                "            if anchors:\n",
                "                self.log(f\"  {field}: {len(anchors)} ancre(s) trouvée(s)\")\n",
                "            else:\n",
                "                self.log(f\"  {field}: aucune ancre trouvée\", \"WARNING\")\n",
                "\n",
                "        return detected\n",
                "\n",
                "    def is_likely_label(self, text: str) -> bool:\n",
                "        \"\"\"\n",
                "        Vérifie si un texte ressemble à un label plutôt qu'à une valeur.\n",
                "        \"\"\"\n",
                "        text_upper = text.upper()\n",
                "\n",
                "        # Vérifier si c'est un format bilingue avec /\n",
                "        if '/' in text and any(word in text_upper for word in ['PERE', 'FATHER', 'MERE', 'MOTHER', \n",
                "                                                                'DATE', 'ADRESSE', 'ADDRESS']):\n",
                "            self.log(f\"    '{text}' identifié comme label (format bilingue avec /)\", \"DEBUG\")\n",
                "            return True\n",
                "\n",
                "        # Vérifier la similarité avec tous les labels connus\n",
                "        for label in self.all_labels:\n",
                "            sim = self.similarity_score(text_upper, label)\n",
                "            if sim >= 0.75:\n",
                "                self.log(f\"    '{text}' identifié comme label (similaire à '{label}', score={sim:.2f})\", \"DEBUG\")\n",
                "                return True\n",
                "\n",
                "        # Mots-clés spécifiques au verso\n",
                "        label_words = ['PERE', 'FATHER', 'MERE', 'MOTHER', 'DATE', 'DELIVRANCE', \n",
                "                      'ISSUE', 'EXPIRATION', 'EXPIRY', 'ADRESSE', 'ADDRESS',\n",
                "                      'POSTE', 'IDENTIFICATION', 'POST', 'IDENTIFIANT', 'UNIQUE',\n",
                "                      'IDENTIFIER', 'AUTORITE', 'AUTHORITY', 'CAMEROUN', 'CAMEROON']\n",
                "\n",
                "        words = text_upper.split()\n",
                "        if len(words) > 1:\n",
                "            matches = sum(1 for word in words if word in label_words)\n",
                "            if matches >= len(words) / 2:\n",
                "                self.log(f\"    '{text}' identifié comme label (mots clés: {matches}/{len(words)})\", \"DEBUG\")\n",
                "                return True\n",
                "\n",
                "        return False\n",
                "\n",
                "    def extract_by_proximity(self, data: List[Tuple[str, float, List]],\n",
                "                            anchor_idx: int, field_name: str) -> Optional[str]:\n",
                "        \"\"\"\n",
                "        Extrait la valeur la plus proche d'une ancre.\n",
                "        \"\"\"\n",
                "        self.log(f\"  Recherche de valeur pour '{field_name}' près de l'ancre à l'index {anchor_idx}\")\n",
                "\n",
                "        if anchor_idx >= len(data):\n",
                "            return None\n",
                "\n",
                "        anchor_text = data[anchor_idx][0]\n",
                "        anchor_polygon = data[anchor_idx][2]\n",
                "        anchor_center = self.calculate_center(anchor_polygon)\n",
                "\n",
                "        candidates = []\n",
                "\n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            if idx == anchor_idx:\n",
                "                continue\n",
                "\n",
                "            if self.is_likely_label(text):\n",
                "                self.log(f\"    Ignoré (label): '{text}'\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            value_center = self.calculate_center(polygon)\n",
                "\n",
                "            # Distance\n",
                "            distance = ((value_center[0] - anchor_center[0])**2 + \n",
                "                       (value_center[1] - anchor_center[1])**2) ** 0.5\n",
                "\n",
                "            # Position relative\n",
                "            is_right = value_center[0] > anchor_center[0]\n",
                "            is_below = value_center[1] > anchor_center[1]\n",
                "\n",
                "            if is_right or is_below:\n",
                "                proximity_score = 1 / (1 + distance/100)\n",
                "                candidates.append({\n",
                "                    'text': text,\n",
                "                    'score': score * proximity_score,\n",
                "                    'distance': distance\n",
                "                })\n",
                "                self.log(f\"    Candidat: '{text}' (distance={distance:.0f}, score={score * proximity_score:.2f})\")\n",
                "\n",
                "        if candidates:\n",
                "            best = max(candidates, key=lambda x: x['score'])\n",
                "            self.log(f\"  Meilleur candidat pour '{field_name}': '{best['text']}'\")\n",
                "            return best['text']\n",
                "\n",
                "        return None\n",
                "\n",
                "    def calculate_center(self, polygon: List[List[int]]) -> Tuple[float, float]:\n",
                "        \"\"\"Calcule le centre d'un polygone.\"\"\"\n",
                "        x_coords = [p[0] for p in polygon]\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        return (sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords))\n",
                "\n",
                "    def extract_remaining_fields(self, data: List[Tuple[str, float, List]], \n",
                "                                anchors: Dict[str, List[Tuple[int, str, float]]],\n",
                "                                fixed_fields: Dict) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait les champs restants (père, mère, adresse, autorité).\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS RESTANTS ===\")\n",
                "\n",
                "        results = {\n",
                "            'pere': None,\n",
                "            'mere': None,\n",
                "            'adresse': None,\n",
                "            'autorite': None,\n",
                "            'poste_identification': fixed_fields.get('poste_code')  # Déjà extrait\n",
                "        }\n",
                "\n",
                "        used_indices = set()\n",
                "        used_values = set()\n",
                "\n",
                "        # Extraction par ancres\n",
                "        for field in ['pere', 'mere', 'adresse', 'autorite']:\n",
                "            if anchors.get(field, []):\n",
                "                best_anchor = max(anchors[field], key=lambda x: x[2])\n",
                "                anchor_idx = best_anchor[0]\n",
                "\n",
                "                self.log(f\"Extraction par ancre pour '{field}':\")\n",
                "                value = self.extract_by_proximity(data, anchor_idx, field)\n",
                "\n",
                "                if value and not self.is_likely_label(value) and value not in used_values:\n",
                "                    results[field] = value\n",
                "                    used_values.add(value)\n",
                "                    self.log(f\"  '{field}' = '{value}' (extrait par ancre)\")\n",
                "\n",
                "        # Pour l'autorité, chercher aussi les noms avec format spécifique\n",
                "        if not results['autorite']:\n",
                "            for text, score, _ in data:\n",
                "                # Pattern pour nom complet (ex: Martin MBARGA NGUELE)\n",
                "                if score > 0.9 and len(text.split()) >= 2:\n",
                "                    words = text.split()\n",
                "                    # Vérifier si c'est un nom propre (mots commençant par majuscule)\n",
                "                    if all(w[0].isupper() for w in words if w):\n",
                "                        if text not in used_values and not self.is_likely_label(text):\n",
                "                            results['autorite'] = text\n",
                "                            self.log(f\"  'autorite' = '{text}' (détection pattern nom)\")\n",
                "                            break\n",
                "\n",
                "        return results\n",
                "\n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, any]:\n",
                "        \"\"\"\n",
                "        Méthode principale d'extraction pour le verso.\n",
                "        \"\"\"\n",
                "        self.log(\"=\"*50)\n",
                "        self.log(\"DÉBUT DE L'EXTRACTION CNI (VERSO)\")\n",
                "        self.log(\"=\"*50)\n",
                "\n",
                "        # 1. Évaluer la qualité\n",
                "        can_proceed, quality_score = self.assess_quality(ocr_data)\n",
                "\n",
                "        if not can_proceed:\n",
                "            return {\n",
                "                'success': False,\n",
                "                'quality_score': quality_score,\n",
                "                'message': 'Qualité OCR insuffisante',\n",
                "                'data': {}\n",
                "            }\n",
                "\n",
                "        # 2. Prétraitement\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        polygons = ocr_data.get('rec_polys', [])\n",
                "\n",
                "        processed_data = self.preprocess(texts, scores, polygons)\n",
                "\n",
                "        # 3. Extraction des champs à format fixe\n",
                "        fixed_fields = self.extract_fixed_format_fields(processed_data)\n",
                "\n",
                "        # Retirer les éléments extraits\n",
                "        remaining_data = [\n",
                "            item for idx, item in enumerate(processed_data)\n",
                "            if idx not in fixed_fields['indices_removed']\n",
                "        ]\n",
                "\n",
                "        self.log(f\"\\nÉléments restants après extraction des champs fixes: {len(remaining_data)}\")\n",
                "\n",
                "        # 4. Détecter les ancres\n",
                "        anchors = self.detect_anchors(remaining_data)\n",
                "\n",
                "        # 5. Extraire les champs restants\n",
                "        remaining_fields = self.extract_remaining_fields(remaining_data, anchors, fixed_fields)\n",
                "\n",
                "        # 6. Consolider les résultats\n",
                "        extracted_data = {\n",
                "            'pere': remaining_fields['pere'],\n",
                "            'mere': remaining_fields['mere'],\n",
                "            'date_delivrance': fixed_fields['date_delivrance'],\n",
                "            'date_expiration': fixed_fields['date_expiration'],\n",
                "            'adresse': remaining_fields['adresse'],\n",
                "            'poste_identification': remaining_fields.get('poste_identification') or fixed_fields.get('poste_code'),\n",
                "            'identifiant_unique': fixed_fields['identifiant_unique'],\n",
                "            'autorite': remaining_fields['autorite'],\n",
                "            'numero_cni': fixed_fields.get('numero_cni')\n",
                "        }\n",
                "\n",
                "        # Score de confiance\n",
                "        filled_fields = sum(1 for v in extracted_data.values() if v is not None)\n",
                "        total_fields = len(extracted_data)\n",
                "        confidence = filled_fields / total_fields if total_fields > 0 else 0\n",
                "\n",
                "        self.log(\"\\n=== RÉSULTATS FINAUX ===\")\n",
                "        for field, value in extracted_data.items():\n",
                "            status = \"[SUCCESS]\" if value else \"[FAILURE]\"\n",
                "            self.log(f\"  {status} {field}: {value}\")\n",
                "        self.log(f\"Confiance: {confidence:.0%}\")\n",
                "\n",
                "        return {\n",
                "            'success': True,\n",
                "            'quality_score': quality_score,\n",
                "            'confidence': confidence,\n",
                "            'anchors_detected': {k: len(v) > 0 for k, v in anchors.items()},\n",
                "            'data': extracted_data\n",
                "        }\n",
                "\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    import json\n",
                "    \n",
                "    # Charger les données OCR du verso\n",
                "    with open('output/images/old/resizedold_back_02_res.json', 'r', encoding='utf-8') as f:\n",
                "        ocr_data = json.load(f)\n",
                "    \n",
                "    # Créer l'extracteur pour le verso\n",
                "    extractor = CNIExtractor18B(debug=True)\n",
                "    \n",
                "    # Extraire les informations\n",
                "    result = extractor.extract(ocr_data)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"RÉSUMÉ DE L'EXTRACTION (VERSO)\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    if result['success']:\n",
                "        print(f\"Extraction réussie (confiance: {result['confidence']:.0%})\")\n",
                "        print(f\"Qualité OCR: {result['quality_score']:.2f}\")\n",
                "        print(\"\\nDonnées extraites:\")\n",
                "        for field, value in result['data'].items():\n",
                "            if value:\n",
                "                print(f\"  {field}: {value}\")\n",
                "    else:\n",
                "        print(f\"Extraction échouée: {result['message']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "75776924",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[INFO] ==================================================\n",
                        "[INFO] DÉBUT DE L'EXTRACTION CNI (FORMAT 2025)\n",
                        "[INFO] ==================================================\n",
                        "[INFO] === ÉVALUATION DE LA QUALITÉ ===\n",
                        "[INFO] Score moyen: 0.93\n",
                        "[INFO] Éléments de bonne qualité (>0.7): 18/20\n",
                        "[INFO] Éléments valides: 19\n",
                        "[INFO] Peut continuer: True\n",
                        "[INFO] === PRÉPROCESSING ===\n",
                        "[DEBUG]   Filtré (score faible): 'å›½' (score=0.18)\n",
                        "[DEBUG]   Filtré (score faible): '' (score=0.00)\n",
                        "[INFO]   Gardé: '100121292' (score=0.95)\n",
                        "[DEBUG]   Filtré (filigrane): 'TRAVAIL'\n",
                        "[INFO]   Gardé: 'NOM/SURNAME' (score=0.99)\n",
                        "[DEBUG]   Filtré (filigrane): 'PATRIE'\n",
                        "[DEBUG]   Filtré (filigrane): 'WORK'\n",
                        "[DEBUG]   Filtré (filigrane): 'FATHERLAND'\n",
                        "[INFO]   Gardé: 'ZYANBA' (score=0.98)\n",
                        "[INFO]   Gardé: 'PRENOMS/GIVEN NAMES' (score=0.98)\n",
                        "[INFO]   Gardé: 'RAHAB' (score=0.99)\n",
                        "[INFO]   Gardé: 'SEXE/SEX' (score=0.98)\n",
                        "[INFO]   Gardé: 'DATE DE NAISSANCE/DATE OF BIRTH' (score=0.95)\n",
                        "[INFO]   Gardé: 'F' (score=0.99)\n",
                        "[INFO]   Gardé: '01.01.1974' (score=0.98)\n",
                        "[INFO]   Gardé: 'DATE D'EXPIRATION/DATE OF EXPIRY' (score=0.96)\n",
                        "[INFO]   Gardé: '27.02.2035' (score=0.99)\n",
                        "[DEBUG]   Filtré (filigrane): 'CMR'\n",
                        "[INFO]   Gardé: 'SIGNATURE/HOLDER'S SIGNATURE' (score=0.96)\n",
                        "[INFO]   Gardé: 'AMEROU' (score=0.92)\n",
                        "[INFO] Éléments après préprocessing: 13/20\n",
                        "[INFO] === EXTRACTION DES CHAMPS À FORMAT FIXE ===\n",
                        "[INFO]   Numéro de carte trouvé: '100121292' à l'index 0\n",
                        "[INFO]   Sexe trouvé: 'F' à l'index 7\n",
                        "[INFO]   Date trouvée: '01.01.1974' à l'index 8\n",
                        "[INFO]   Date trouvée: '27.02.2035' à l'index 10\n",
                        "[INFO]   Date de naissance identifiée: 01.01.1974 (année 1974)\n",
                        "[INFO]   Date d'expiration identifiée: 27.02.2035 (année 2035)\n",
                        "[INFO] Champs fixes extraits: 4 sur 4\n",
                        "[INFO] Indices à retirer: [0, 7, 8, 10]\n",
                        "[INFO] \n",
                        "Éléments restants après extraction des champs fixes: 9\n",
                        "[INFO]   - 'NOM/SURNAME' (score=0.99)\n",
                        "[INFO]   - 'ZYANBA' (score=0.98)\n",
                        "[INFO]   - 'PRENOMS/GIVEN NAMES' (score=0.98)\n",
                        "[INFO]   - 'RAHAB' (score=0.99)\n",
                        "[INFO]   - 'SEXE/SEX' (score=0.98)\n",
                        "[INFO]   - 'DATE DE NAISSANCE/DATE OF BIRTH' (score=0.95)\n",
                        "[INFO]   - 'DATE D'EXPIRATION/DATE OF EXPIRY' (score=0.96)\n",
                        "[INFO]   - 'SIGNATURE/HOLDER'S SIGNATURE' (score=0.96)\n",
                        "[INFO]   - 'AMEROU' (score=0.92)\n",
                        "[INFO] === DÉTECTION DES ANCRES ===\n",
                        "[INFO]   Ancre détectée pour 'nom': 'NOM/SURNAME' ~ 'SURNAME' (similarité=0.78)\n",
                        "[INFO]   Ancre détectée pour 'prenom': 'PRENOMS/GIVEN NAMES' ~ 'PRENOMS' (similarité=0.72)\n",
                        "[INFO]   Ancre détectée pour 'sexe': 'SEXE/SEX' ~ 'SEXE' (similarité=0.80)\n",
                        "[INFO]   Ancre détectée pour 'date_naissance': 'DATE DE NAISSANCE/DATE OF BIRTH' ~ 'DATE DE NAISSANCE' (similarité=0.83)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATE DE NAISSANCE/DATE OF BIRTH' ~ 'DATE D'EXPIRATION/DATE OF EXPIRY' (similarité=0.76)\n",
                        "[INFO]   Ancre détectée pour 'date_naissance': 'DATE D'EXPIRATION/DATE OF EXPIRY' ~ 'DATE DE NAISSANCE/DATE OF BIRTH' (similarité=0.80)\n",
                        "[INFO]   Ancre détectée pour 'date_expiration': 'DATE D'EXPIRATION/DATE OF EXPIRY' ~ 'DATE D'EXPIRATION' (similarité=0.82)\n",
                        "[INFO]   Ancre détectée pour 'signature': 'SIGNATURE/HOLDER'S SIGNATURE' ~ 'HOLDER'S SIGNATURE' (similarité=0.78)\n",
                        "[INFO]   nom: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   prenom: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   date_naissance: 2 ancre(s) trouvée(s)\n",
                        "[INFO]   date_expiration: 2 ancre(s) trouvée(s)\n",
                        "[INFO]   sexe: 1 ancre(s) trouvée(s)\n",
                        "[INFO]   signature: 1 ancre(s) trouvée(s)\n",
                        "[INFO] === EXTRACTION DES CHAMPS RESTANTS ===\n",
                        "[INFO] Extraction par ancre pour 'nom':\n",
                        "[INFO]   Recherche de valeur pour 'nom' près de l'ancre à l'index 0\n",
                        "[INFO]     Candidat: 'ZYANBA' (distance=65, score=0.59)\n",
                        "[DEBUG]     'PRENOMS/GIVEN NAMES' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'PRENOMS/GIVEN NAMES'\n",
                        "[INFO]     Candidat: 'RAHAB' (distance=204, score=0.32)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[DEBUG]     'DATE D'EXPIRATION/DATE OF EXPIRY' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE D'EXPIRATION/DATE OF EXPIRY'\n",
                        "[DEBUG]     'SIGNATURE/HOLDER'S SIGNATURE' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE/HOLDER'S SIGNATURE'\n",
                        "[DEBUG]     'AMEROU' identifié comme label (similaire à 'CAMEROUN', score=0.86)\n",
                        "[DEBUG]     Ignoré (label): 'AMEROU'\n",
                        "[INFO]   Meilleur candidat pour 'nom': 'ZYANBA'\n",
                        "[INFO]   'nom' = 'ZYANBA' (extrait par ancre)\n",
                        "[INFO] Extraction par ancre pour 'prenom':\n",
                        "[INFO]   Recherche de valeur pour 'prenom' près de l'ancre à l'index 2\n",
                        "[DEBUG]     'NOM/SURNAME' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'NOM/SURNAME'\n",
                        "[INFO]     Candidat: 'RAHAB' (distance=155, score=0.39)\n",
                        "[DEBUG]     'SEXE/SEX' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SEXE/SEX'\n",
                        "[DEBUG]     'DATE DE NAISSANCE/DATE OF BIRTH' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE DE NAISSANCE/DATE OF BIRTH'\n",
                        "[DEBUG]     'DATE D'EXPIRATION/DATE OF EXPIRY' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'DATE D'EXPIRATION/DATE OF EXPIRY'\n",
                        "[DEBUG]     'SIGNATURE/HOLDER'S SIGNATURE' identifié comme label (format bilingue avec /)\n",
                        "[DEBUG]     Ignoré (label): 'SIGNATURE/HOLDER'S SIGNATURE'\n",
                        "[DEBUG]     'AMEROU' identifié comme label (similaire à 'CAMEROUN', score=0.86)\n",
                        "[DEBUG]     Ignoré (label): 'AMEROU'\n",
                        "[INFO]   Meilleur candidat pour 'prenom': 'RAHAB'\n",
                        "[INFO]   'prenom' = 'RAHAB' (extrait par ancre)\n",
                        "[INFO] \n",
                        "=== RÉSULTATS FINAUX ===\n",
                        "[INFO]   [SUCCESS] numero_carte: 100121292\n",
                        "[INFO]   [SUCCESS] nom: ZYANBA\n",
                        "[INFO]   [SUCCESS] prenom: RAHAB\n",
                        "[INFO]   [SUCCESS] date_naissance: 01.01.1974\n",
                        "[INFO]   [SUCCESS] date_expiration: 27.02.2035\n",
                        "[INFO]   [SUCCESS] sexe: F\n",
                        "[INFO] Confiance: 100%\n",
                        "\n",
                        "==================================================\n",
                        "RÉSUMÉ DE L'EXTRACTION (FORMAT 2025)\n",
                        "==================================================\n",
                        "Extraction réussie (confiance: 100%)\n",
                        "Qualité OCR: 0.93\n",
                        "\n",
                        "Données extraites:\n",
                        "  numero_carte: 100121292\n",
                        "  nom: ZYANBA\n",
                        "  prenom: RAHAB\n",
                        "  date_naissance: 01.01.1974\n",
                        "  date_expiration: 27.02.2035\n",
                        "  sexe: F\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "\n",
                "class CNIExtractor2025F:\n",
                "    \"\"\"\n",
                "    Extracteur pour le nouveau format de CNI camerounaise (2025) \n",
                "    utilisant une approche par élimination avec détection floue des ancres.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, quality_threshold: float = 0.5, similarity_threshold: float = 0.70, debug: bool = True):\n",
                "        \"\"\"\n",
                "        Initialise l'extracteur pour le nouveau format.\n",
                "\n",
                "        Args:\n",
                "            quality_threshold: Seuil minimal de qualité OCR pour procéder\n",
                "            similarity_threshold: Seuil de similarité pour la détection des ancres\n",
                "            debug: Active les logs de débogage\n",
                "        \"\"\"\n",
                "        self.quality_threshold = quality_threshold\n",
                "        self.similarity_threshold = similarity_threshold\n",
                "        self.debug = debug\n",
                "\n",
                "        # Ancres possibles pour chaque champ\n",
                "        self.anchors = {\n",
                "            'nom': ['NOM', 'SURNAME', 'NOM/SURNAME'],\n",
                "            'prenom': ['PRENOMS', 'PRENOM', 'GIVEN NAMES', 'GIVEN NAME', \n",
                "                      'PRENOMS/GIVEN NAMES', 'PRENOMS/GIVEN NAME'],\n",
                "            'date_naissance': ['DATE DE NAISSANCE', 'DATE OF BIRTH', \n",
                "                             'DATE DENAISSANCE', 'DATEOF BIRTH',\n",
                "                             'DATE DE NAISSANCE/DATE OF BIRTH'],\n",
                "            'date_expiration': ['DATE D\\'EXPIRATION', 'DATE OF EXPIRY',\n",
                "                              'DATED\\'EXPIRATION', 'DATEOF EXPIRY',\n",
                "                              'DATE D\\'EXPIRATION/DATE OF EXPIRY'],\n",
                "            'sexe': ['SEXE', 'SEX', 'SEXE/SEX'],\n",
                "            'signature': ['SIGNATURE', 'HOLDER\\'S SIGNATURE', \n",
                "                         'SIGNATURE/HOLDER\\'S SIGNATURE']\n",
                "        }\n",
                "\n",
                "        # Labels à ignorer (filigranes et textes de fond)\n",
                "        self.ignore_words = {\n",
                "            'TRAVAIL', 'PATRIE', 'WORK', 'FATHERLAND', \n",
                "            'CMR', 'CAMEROUN', 'CAMEROON',\n",
                "            'REPUBLIQUE', 'REPUBLIC', 'DU', 'OF',\n",
                "            'CARTE', 'NATIONALE', 'IDENTITE', \n",
                "            'NATIONAL', 'IDENTITY', 'CARD'\n",
                "        }\n",
                "\n",
                "        # Tous les labels possibles (pour filtrage)\n",
                "        self.all_labels = set()\n",
                "        for labels in self.anchors.values():\n",
                "            self.all_labels.update(labels)\n",
                "        self.all_labels.update(self.ignore_words)\n",
                "        self.all_labels.update(['SIGNATURE', 'HOLDER\\'S'])\n",
                "\n",
                "    def log(self, message: str, level: str = \"INFO\"):\n",
                "        \"\"\"Affiche un message de log si le mode debug est activé.\"\"\"\n",
                "        if self.debug:\n",
                "            print(f\"[{level}] {message}\")\n",
                "\n",
                "    def assess_quality(self, ocr_data: Dict) -> Tuple[bool, float]:\n",
                "        \"\"\"\n",
                "        Évalue la qualité globale des données OCR.\n",
                "\n",
                "        Returns:\n",
                "            (peut_continuer, score_qualite)\n",
                "        \"\"\"\n",
                "        self.log(\"=== ÉVALUATION DE LA QUALITÉ ===\")\n",
                "\n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "\n",
                "        if not scores or not texts:\n",
                "            self.log(\"Pas de données OCR\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "\n",
                "        # Filtrer les scores valides (> 0)\n",
                "        valid_scores = [s for s in scores if s > 0]\n",
                "\n",
                "        if not valid_scores:\n",
                "            self.log(\"Aucun score valide\", \"ERROR\")\n",
                "            return False, 0.0\n",
                "\n",
                "        # Calculer le score moyen\n",
                "        avg_score = sum(valid_scores) / len(valid_scores)\n",
                "\n",
                "        # Compter les éléments de bonne qualité\n",
                "        good_quality = sum(1 for s in scores if s > 0.7)\n",
                "\n",
                "        self.log(f\"Score moyen: {avg_score:.2f}\")\n",
                "        self.log(f\"Éléments de bonne qualité (>0.7): {good_quality}/{len(scores)}\")\n",
                "        self.log(f\"Éléments valides: {len(valid_scores)}\")\n",
                "\n",
                "        # Au moins 6 éléments détectés pour le nouveau format\n",
                "        can_proceed = (len(valid_scores) >= 6 and \n",
                "                      avg_score >= self.quality_threshold and\n",
                "                      good_quality >= 4)\n",
                "\n",
                "        self.log(f\"Peut continuer: {can_proceed}\")\n",
                "\n",
                "        return can_proceed, avg_score\n",
                "\n",
                "    def preprocess(self, texts: List[str], scores: List[float],\n",
                "                  polygons: List) -> List[Tuple[str, float, List]]:\n",
                "        \"\"\"\n",
                "        Prétraite les données OCR en filtrant le bruit et les filigranes.\n",
                "        \"\"\"\n",
                "        self.log(\"=== PRÉPROCESSING ===\")\n",
                "        processed = []\n",
                "\n",
                "        for i, (text, score, polygon) in enumerate(zip(texts, scores, polygons)):\n",
                "            # Filtrer les scores très faibles\n",
                "            if score < 0.3:\n",
                "                self.log(f\"  Filtré (score faible): '{text}' (score={score:.2f})\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            # Filtrer les textes vides\n",
                "            text = text.strip()\n",
                "            if not text:\n",
                "                self.log(f\"  Filtré (texte vide): index {i}\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            # Filtrer les caractères non-latins isolés (国, etc.)\n",
                "            if len(text) <= 2:\n",
                "                if any(ord(c) > 127 for c in text):\n",
                "                    self.log(f\"  Filtré (caractère non-latin): '{text}'\", \"DEBUG\")\n",
                "                    continue\n",
                "\n",
                "            # Filtrer les mots isolés qui sont des filigranes connus\n",
                "            if text.upper() in self.ignore_words:\n",
                "                self.log(f\"  Filtré (filigrane): '{text}'\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            # Filtrer CMR et codes pays de 3 lettres\n",
                "            if len(text) == 3 and text.isupper() and text.isalpha():\n",
                "                self.log(f\"  Filtré (code pays probable): '{text}'\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            processed.append((text, score, polygon))\n",
                "            self.log(f\"  Gardé: '{text}' (score={score:.2f})\")\n",
                "\n",
                "        self.log(f\"Éléments après préprocessing: {len(processed)}/{len(texts)}\")\n",
                "\n",
                "        return processed\n",
                "\n",
                "    def similarity_score(self, str1: str, str2: str) -> float:\n",
                "        \"\"\"\n",
                "        Calcule le score de similarité entre deux chaînes.\n",
                "        \"\"\"\n",
                "        s1 = str1.upper().strip()\n",
                "        s2 = str2.upper().strip()\n",
                "\n",
                "        base_score = SequenceMatcher(None, s1, s2).ratio()\n",
                "\n",
                "        # Bonus pour préfixe identique\n",
                "        prefix_match = 0\n",
                "        for i in range(min(4, len(s1), len(s2))):\n",
                "            if s1[i] == s2[i]:\n",
                "                prefix_match += 1\n",
                "            else:\n",
                "                break\n",
                "\n",
                "        final_score = base_score + (prefix_match * 0.1 * (1 - base_score))\n",
                "\n",
                "        return min(final_score, 1.0)\n",
                "\n",
                "    def extract_fixed_format_fields(self, data: List[Tuple[str, float, List]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait les champs à format fixe (numéro, dates, sexe).\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS À FORMAT FIXE ===\")\n",
                "\n",
                "        results = {\n",
                "            'numero_carte': None,\n",
                "            'date_naissance': None,\n",
                "            'date_expiration': None,\n",
                "            'sexe': None\n",
                "        }\n",
                "        indices_to_remove = []\n",
                "\n",
                "        # Patterns de détection\n",
                "        date_pattern = re.compile(r'^\\d{1,2}\\.\\d{1,2}\\.\\d{4}$')  # Format avec points\n",
                "        numero_pattern = re.compile(r'^\\d{9}$')  # Numéro de carte (9 chiffres)\n",
                "\n",
                "        dates_found = []\n",
                "        \n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            # Numéro de carte (9 chiffres)\n",
                "            if not results['numero_carte'] and numero_pattern.match(text):\n",
                "                self.log(f\"  Numéro de carte trouvé: '{text}' à l'index {idx}\")\n",
                "                results['numero_carte'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Dates (format avec points)\n",
                "            if date_pattern.match(text):\n",
                "                dates_found.append((text, idx, polygon))\n",
                "                self.log(f\"  Date trouvée: '{text}' à l'index {idx}\")\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "            \n",
                "            # Sexe\n",
                "            if not results['sexe'] and text in ['M', 'F']:\n",
                "                self.log(f\"  Sexe trouvé: '{text}' à l'index {idx}\")\n",
                "                results['sexe'] = text\n",
                "                indices_to_remove.append(idx)\n",
                "                continue\n",
                "\n",
                "        # Distinguer les dates par leur année\n",
                "        for date_text, idx, polygon in dates_found:\n",
                "            year = int(date_text.split('.')[-1])\n",
                "            \n",
                "            # Date de naissance : année < 2010 généralement\n",
                "            if not results['date_naissance'] and year < 2010:\n",
                "                results['date_naissance'] = date_text\n",
                "                self.log(f\"  Date de naissance identifiée: {date_text} (année {year})\")\n",
                "            \n",
                "            # Date d'expiration : année > 2020 généralement\n",
                "            elif not results['date_expiration'] and year > 2020:\n",
                "                results['date_expiration'] = date_text\n",
                "                self.log(f\"  Date d'expiration identifiée: {date_text} (année {year})\")\n",
                "\n",
                "        # Si on n'a pas pu distinguer par l'année, prendre par ordre\n",
                "        if not results['date_naissance'] and dates_found:\n",
                "            results['date_naissance'] = dates_found[0][0]\n",
                "        if not results['date_expiration'] and len(dates_found) > 1:\n",
                "            results['date_expiration'] = dates_found[1][0]\n",
                "\n",
                "        self.log(f\"Champs fixes extraits: {len([v for v in results.values() if v])} sur 4\")\n",
                "        self.log(f\"Indices à retirer: {indices_to_remove}\")\n",
                "\n",
                "        results['indices_removed'] = indices_to_remove\n",
                "\n",
                "        return results\n",
                "\n",
                "    def detect_anchors(self, data: List[Tuple[str, float, List]]) -> Dict[str, List[Tuple[int, str, float]]]:\n",
                "        \"\"\"\n",
                "        Détecte les ancres avec similarité floue.\n",
                "        \"\"\"\n",
                "        self.log(\"=== DÉTECTION DES ANCRES ===\")\n",
                "\n",
                "        detected = {field: [] for field in self.anchors}\n",
                "\n",
                "        for idx, (text, score, _) in enumerate(data):\n",
                "            text_upper = text.upper()\n",
                "\n",
                "            for field, anchor_list in self.anchors.items():\n",
                "                for anchor in anchor_list:\n",
                "                    sim_score = self.similarity_score(text_upper, anchor)\n",
                "\n",
                "                    if sim_score >= self.similarity_threshold:\n",
                "                        self.log(f\"  Ancre détectée pour '{field}': '{text}' ~ '{anchor}' (similarité={sim_score:.2f})\")\n",
                "                        detected[field].append((idx, text, sim_score))\n",
                "                        break\n",
                "\n",
                "        # Résumé des ancres détectées\n",
                "        for field, anchors in detected.items():\n",
                "            if anchors:\n",
                "                self.log(f\"  {field}: {len(anchors)} ancre(s) trouvée(s)\")\n",
                "            else:\n",
                "                self.log(f\"  {field}: aucune ancre trouvée\", \"WARNING\")\n",
                "\n",
                "        return detected\n",
                "\n",
                "    def is_likely_label(self, text: str) -> bool:\n",
                "        \"\"\"\n",
                "        Vérifie si un texte ressemble à un label plutôt qu'à une valeur.\n",
                "        \"\"\"\n",
                "        text_upper = text.upper()\n",
                "\n",
                "        # Vérifier si c'est un mot isolé de filigrane\n",
                "        if text_upper in self.ignore_words:\n",
                "            self.log(f\"    '{text}' identifié comme filigrane\", \"DEBUG\")\n",
                "            return True\n",
                "\n",
                "        # Vérifier si c'est un format bilingue avec /\n",
                "        if '/' in text and any(word in text_upper for word in ['NOM', 'SURNAME', 'PRENOM', 'GIVEN', \n",
                "                                                                'DATE', 'SEXE', 'SEX', 'SIGNATURE']):\n",
                "            self.log(f\"    '{text}' identifié comme label (format bilingue avec /)\", \"DEBUG\")\n",
                "            return True\n",
                "\n",
                "        # Vérifier la similarité avec tous les labels connus\n",
                "        for label in self.all_labels:\n",
                "            sim = self.similarity_score(text_upper, label)\n",
                "            if sim >= 0.75:\n",
                "                self.log(f\"    '{text}' identifié comme label (similaire à '{label}', score={sim:.2f})\", \"DEBUG\")\n",
                "                return True\n",
                "\n",
                "        # Mots-clés spécifiques\n",
                "        label_words = ['NOM', 'SURNAME', 'PRENOMS', 'PRENOM', 'GIVEN', 'NAMES', 'NAME',\n",
                "                      'DATE', 'NAISSANCE', 'BIRTH', 'EXPIRATION', 'EXPIRY',\n",
                "                      'SEXE', 'SEX', 'SIGNATURE', 'HOLDER\\'S']\n",
                "\n",
                "        words = text_upper.split()\n",
                "        if len(words) > 1:\n",
                "            matches = sum(1 for word in words if word in label_words)\n",
                "            if matches >= len(words) / 2:\n",
                "                self.log(f\"    '{text}' identifié comme label (mots clés: {matches}/{len(words)})\", \"DEBUG\")\n",
                "                return True\n",
                "\n",
                "        return False\n",
                "\n",
                "    def extract_by_proximity(self, data: List[Tuple[str, float, List]],\n",
                "                            anchor_idx: int, field_name: str) -> Optional[str]:\n",
                "        \"\"\"\n",
                "        Extrait la valeur la plus proche d'une ancre.\n",
                "        \"\"\"\n",
                "        self.log(f\"  Recherche de valeur pour '{field_name}' près de l'ancre à l'index {anchor_idx}\")\n",
                "\n",
                "        if anchor_idx >= len(data):\n",
                "            return None\n",
                "\n",
                "        anchor_text = data[anchor_idx][0]\n",
                "        anchor_polygon = data[anchor_idx][2]\n",
                "        anchor_center = self.calculate_center(anchor_polygon)\n",
                "\n",
                "        candidates = []\n",
                "\n",
                "        for idx, (text, score, polygon) in enumerate(data):\n",
                "            if idx == anchor_idx:\n",
                "                continue\n",
                "\n",
                "            if self.is_likely_label(text):\n",
                "                self.log(f\"    Ignoré (label): '{text}'\", \"DEBUG\")\n",
                "                continue\n",
                "\n",
                "            value_center = self.calculate_center(polygon)\n",
                "\n",
                "            # Distance\n",
                "            distance = ((value_center[0] - anchor_center[0])**2 + \n",
                "                       (value_center[1] - anchor_center[1])**2) ** 0.5\n",
                "\n",
                "            # Position relative\n",
                "            is_right = value_center[0] > anchor_center[0]\n",
                "            is_below = value_center[1] > anchor_center[1]\n",
                "\n",
                "            if is_right or is_below:\n",
                "                proximity_score = 1 / (1 + distance/100)\n",
                "                candidates.append({\n",
                "                    'text': text,\n",
                "                    'score': score * proximity_score,\n",
                "                    'distance': distance\n",
                "                })\n",
                "                self.log(f\"    Candidat: '{text}' (distance={distance:.0f}, score={score * proximity_score:.2f})\")\n",
                "\n",
                "        if candidates:\n",
                "            best = max(candidates, key=lambda x: x['score'])\n",
                "            self.log(f\"  Meilleur candidat pour '{field_name}': '{best['text']}'\")\n",
                "            return best['text']\n",
                "\n",
                "        return None\n",
                "\n",
                "    def calculate_center(self, polygon: List[List[int]]) -> Tuple[float, float]:\n",
                "        \"\"\"Calcule le centre d'un polygone.\"\"\"\n",
                "        x_coords = [p[0] for p in polygon]\n",
                "        y_coords = [p[1] for p in polygon]\n",
                "        return (sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords))\n",
                "\n",
                "    def extract_remaining_fields(self, data: List[Tuple[str, float, List]], \n",
                "                                anchors: Dict[str, List[Tuple[int, str, float]]]) -> Dict:\n",
                "        \"\"\"\n",
                "        Extrait les champs restants (nom, prénom).\n",
                "        \"\"\"\n",
                "        self.log(\"=== EXTRACTION DES CHAMPS RESTANTS ===\")\n",
                "\n",
                "        results = {\n",
                "            'nom': None,\n",
                "            'prenom': None\n",
                "        }\n",
                "\n",
                "        used_values = set()\n",
                "\n",
                "        # Extraction par ancres\n",
                "        for field in ['nom', 'prenom']:\n",
                "            if anchors.get(field, []):\n",
                "                best_anchor = max(anchors[field], key=lambda x: x[2])\n",
                "                anchor_idx = best_anchor[0]\n",
                "\n",
                "                self.log(f\"Extraction par ancre pour '{field}':\")\n",
                "                value = self.extract_by_proximity(data, anchor_idx, field)\n",
                "\n",
                "                if value and not self.is_likely_label(value) and value not in used_values:\n",
                "                    results[field] = value\n",
                "                    used_values.add(value)\n",
                "                    self.log(f\"  '{field}' = '{value}' (extrait par ancre)\")\n",
                "\n",
                "        # Si les champs manquent, chercher les textes restants valides\n",
                "        if not results['nom'] or not results['prenom']:\n",
                "            remaining_texts = []\n",
                "            for text, score, polygon in data:\n",
                "                # Vérifier que c'est un nom valide (alphabétique, score élevé)\n",
                "                if (text not in used_values and \n",
                "                    not self.is_likely_label(text) and\n",
                "                    score > 0.9 and\n",
                "                    text.isalpha() and\n",
                "                    len(text) > 2):\n",
                "                    \n",
                "                    y_pos = self.calculate_center(polygon)[1]\n",
                "                    remaining_texts.append({\n",
                "                        'text': text,\n",
                "                        'score': score,\n",
                "                        'y_position': y_pos\n",
                "                    })\n",
                "                    self.log(f\"  Texte candidat nom/prénom: '{text}' (y={y_pos:.0f}, score={score:.2f})\")\n",
                "\n",
                "            # Trier par position verticale\n",
                "            remaining_texts.sort(key=lambda x: x['y_position'])\n",
                "\n",
                "            # Assigner les champs manquants\n",
                "            if not results['nom'] and remaining_texts:\n",
                "                results['nom'] = remaining_texts[0]['text']\n",
                "                self.log(f\"  'nom' = '{results['nom']}' (position: premier élément)\")\n",
                "                remaining_texts.pop(0)\n",
                "\n",
                "            if not results['prenom'] and remaining_texts:\n",
                "                results['prenom'] = remaining_texts[0]['text']\n",
                "                self.log(f\"  'prenom' = '{results['prenom']}' (position: deuxième élément)\")\n",
                "\n",
                "        return results\n",
                "\n",
                "    def extract(self, ocr_data: Dict) -> Dict[str, any]:\n",
                "        \"\"\"\n",
                "        Méthode principale d'extraction pour le nouveau format.\n",
                "        \"\"\"\n",
                "        self.log(\"=\"*50)\n",
                "        self.log(\"DÉBUT DE L'EXTRACTION CNI (FORMAT 2025)\")\n",
                "        self.log(\"=\"*50)\n",
                "\n",
                "        # 1. Évaluer la qualité\n",
                "        can_proceed, quality_score = self.assess_quality(ocr_data)\n",
                "\n",
                "        if not can_proceed:\n",
                "            return {\n",
                "                'success': False,\n",
                "                'quality_score': quality_score,\n",
                "                'message': 'Qualité OCR insuffisante',\n",
                "                'data': {}\n",
                "            }\n",
                "\n",
                "        # 2. Prétraitement\n",
                "        texts = ocr_data.get('rec_texts', [])\n",
                "        scores = ocr_data.get('rec_scores', [])\n",
                "        polygons = ocr_data.get('rec_polys', [])\n",
                "\n",
                "        processed_data = self.preprocess(texts, scores, polygons)\n",
                "\n",
                "        # 3. Extraction des champs à format fixe\n",
                "        fixed_fields = self.extract_fixed_format_fields(processed_data)\n",
                "\n",
                "        # Retirer les éléments extraits\n",
                "        remaining_data = [\n",
                "            item for idx, item in enumerate(processed_data)\n",
                "            if idx not in fixed_fields['indices_removed']\n",
                "        ]\n",
                "\n",
                "        self.log(f\"\\nÉléments restants après extraction des champs fixes: {len(remaining_data)}\")\n",
                "        for text, score, _ in remaining_data:\n",
                "            self.log(f\"  - '{text}' (score={score:.2f})\")\n",
                "\n",
                "        # 4. Détecter les ancres\n",
                "        anchors = self.detect_anchors(remaining_data)\n",
                "\n",
                "        # 5. Extraire les champs restants\n",
                "        remaining_fields = self.extract_remaining_fields(remaining_data, anchors)\n",
                "\n",
                "        # 6. Consolider les résultats (gardant les mêmes clés pour la cohérence)\n",
                "        extracted_data = {\n",
                "            'numero_carte': fixed_fields['numero_carte'],\n",
                "            'nom': remaining_fields['nom'],\n",
                "            'prenom': remaining_fields['prenom'],\n",
                "            'date_naissance': fixed_fields['date_naissance'],\n",
                "            'date_expiration': fixed_fields['date_expiration'],\n",
                "            'sexe': fixed_fields['sexe']\n",
                "        }\n",
                "\n",
                "        # Score de confiance\n",
                "        filled_fields = sum(1 for v in extracted_data.values() if v is not None)\n",
                "        total_fields = len(extracted_data)\n",
                "        confidence = filled_fields / total_fields if total_fields > 0 else 0\n",
                "\n",
                "        self.log(\"\\n=== RÉSULTATS FINAUX ===\")\n",
                "        for field, value in extracted_data.items():\n",
                "            status = \"[SUCCESS]\" if value else \"[FAILURE]\"\n",
                "            self.log(f\"  {status} {field}: {value}\")\n",
                "        self.log(f\"Confiance: {confidence:.0%}\")\n",
                "\n",
                "        return {\n",
                "            'success': True,\n",
                "            'quality_score': quality_score,\n",
                "            'confidence': confidence,\n",
                "            'anchors_detected': {k: len(v) > 0 for k, v in anchors.items()},\n",
                "            'data': extracted_data\n",
                "        }\n",
                "\n",
                "\n",
                "# Exemple d'utilisation\n",
                "if __name__ == \"__main__\":\n",
                "    import json\n",
                "    \n",
                "    # Charger les données OCR du nouveau format\n",
                "    with open('output/images/new_front_02_res.json', 'r') as f:\n",
                "        ocr_data = json.load(f)\n",
                "    \n",
                "    # Créer l'extracteur pour le nouveau format\n",
                "    extractor = CNIExtractor2025F(debug=True)\n",
                "    \n",
                "    # Extraire les informations\n",
                "    result = extractor.extract(ocr_data)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"RÉSUMÉ DE L'EXTRACTION (FORMAT 2025)\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    if result['success']:\n",
                "        print(f\"Extraction réussie (confiance: {result['confidence']:.0%})\")\n",
                "        print(f\"Qualité OCR: {result['quality_score']:.2f}\")\n",
                "        print(\"\\nDonnées extraites:\")\n",
                "        for field, value in result['data'].items():\n",
                "            if value:\n",
                "                print(f\"  {field}: {value}\")\n",
                "    else:\n",
                "        print(f\"Extraction échouée: {result['message']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f3e5075",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
